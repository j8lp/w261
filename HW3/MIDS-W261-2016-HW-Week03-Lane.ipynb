{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 3\n",
    "Jackson Lane (jelane@berkeley.edu) <br>\n",
    "W261-3 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.  \n",
    "##### How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you're starting out with two sorted lists, it's pretty easy to merge them.  First initalize a new empty list to hold all the elements from your two input lists.  Then out of all the elements in the two input lists, remove the element with the smallest key and put it at the begining of the new list.  Then out of the remaining elements from the two input lists, take the element with the smallest and put it in the second position in the new list.  Repeat this step until you have no more elements left in either input list.  You should now have a sorted list.\n",
    "\n",
    "Because the two input lists are already sorted, you only need to make one comparison per step to find the smallest key.  As such, the algorithm can run in O(n+m) time, where n and m are the size of the first and second input lists respectively.  \n",
    "\n",
    "The interactive HTML below does a better job of explaining the process than I do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div style=\"width:700px; height:400px; overflow:hidden;  position:relative; margin: 0px auto; background-color: #FFF\">\n",
    "<iframe src=\"http://cs.armstrong.edu/liang/animation/web/MergeList.html\" scrolling=\"no\" style=\"overflow:hidden; border:0px; position:absolute; top:-100px; left:0px; width: 700px; height:450px;\"></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hadoop, this type of sort is applied after the combining phase in the Hadoop shuffle.  If there is more than 1 mapper, then Hadoop will merge the outputs of the two mappers and/or combiners together on the reducer before executing the actual reducer code.  Hadoop can do this, because it assumes that both of these outputs were already sorted during the shuffle.\n",
    "This type of sorting also happens during the Hadoop shuffle multiple times in a process called \"merge-sort\" that takes two <b>unsorted</b> lists and returns a sorted list.  The merge-sort process happens during the shuffle phasse, after partitioning but before combining.  It works by breaking down the lists into single elements and then merging these single elements back into larger sorted lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is  a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combiner function is like a reducer run on the mapper side.  It happens during the shuffle phase and runs on elements with the same key.  It's used to reduce the volumne of data sent from the mappers to the reducers over the network.  A good example is in the generic word count example.  Basically, instead of sending over 50 instances of 1 count of the same word for the reducer to count, you can use a combiner to aggregate those 50 instances into just 1 instance with a count of 50.  In java the combiner may be applied once per unique key emitted by the mappers.  However, in Hadoop Streaming, it appears that combiner can sometimes run on multiple keys at a time.  In both cases however, Hadoop does not guarantee that it will run the combiner or the amount of times it will run the combiner.  It could run 0, 1, or many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadoop Shuffle is the process Hadoop uses to transfer the outputs from the mappers to reducers.  The shuffle consists of three parts:  Partitioning, sorting, and combining. The first two phases always happen in that order in each shuffle phase, but the third phase may or may not happen, hence why Hadoop does not guarantee it will run combiners.  Partitioning divides up the mapper outputs among the reducer tasks.  Typically this is done using a hash, but you can specify other types of partitioners as well in the job configuration.  Once partitioning has completed, the sorting phase sorts each partition using merge-sort.  Again, this behavior is configurable in the job configuration.  Then the combiner phase may execute a combiner funciton on the sorted partitions.  Note that the combiner cannot change which reducer a record gets sent to, as that is determined in the partitioning pase.  But a combiner change the order in which the reducer recieves the records by printing the records in a different order than received from the sort phase.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 \n",
    "\n",
    "Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author:Jackson Lane\n",
    "# Description: mapper code for HW3.1\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    product = fields[1]\n",
    "    #Group all other product fields into just \"other\"\n",
    "    if product != \"Debt collection\" and product != \"Mortgage\": product = \"Other\"\n",
    "    #Increment counter by 1\n",
    "    sys.stderr.write(\"reporter:counter:3.1,\"+product+\",1\\n\")\n",
    "    print product,\",\", 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python \n",
    "# reducer.py\n",
    "# Author: Jackson Lane\n",
    "# Description: reducer code for HW3.1\n",
    "\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "word = \"\"\n",
    "count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    newword, newcount = line.split(\",\")\n",
    "    newcount = 1\n",
    "    if (newword == word): count += newcount\n",
    "    else:\n",
    "        # We have finished with all instances of the current word.  \n",
    "        # Print total count and move on to next word\n",
    "        if (count > 0): print (word, count ,sep=',')\n",
    "        word = newword\n",
    "        count = newcount\n",
    "if (count > 0): print (word, count , sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mapreduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r results/3.1\n",
    "!hdfs dfs -put Consumer_Complaints.csv \n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=-n \\\n",
    "-D mapreduce.output.key.field.separator=\",\" \\\n",
    "-file mapper.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output results/3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat results/3.1/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://dl.dropboxusercontent.com/u/43045211/stats/HW3/HW3.1.png.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 \n",
    "Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2.0.1\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author:Jackson Lane\n",
    "# Description: mapper code for HW3.2.0.1\n",
    "\n",
    "import sys\n",
    "sys.stderr.write(\"reporter:counter:3.2,MapperCount,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        #Emit each word with a count of 1\n",
    "        print word,\",\",1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python \n",
    "# reducer.py\n",
    "# Author: Jackson Lane\n",
    "# Description: reducer code for 3.2.0.1\n",
    "# Same as reducer code for 3.1, but with a counter\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "word = \"\"\n",
    "count = 0\n",
    "sys.stderr.write(\"reporter:counter:3.2,ReducerCount,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    newword, newcount = line.split(\",\")\n",
    "    newcount = int(newcount)\n",
    "    if (newword == word): count += newcount\n",
    "    else:\n",
    "        # We have finished with all instances of the current word.  \n",
    "        # Print total count and move on to next word\n",
    "        if (count > 0): print (word, count, sep=',')\n",
    "        word = newword\n",
    "        count = newcount\n",
    "if (count > 0): print (word,  count, sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile ffqlfbq.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mapreduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r results/3.2\n",
    "!hdfs dfs -put -p -f ffqlfbq.txt\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-file mapper.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-input ffqlfbq.txt \\\n",
    "-output results/3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat results/3.2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper and reducer counters are equal to the number of mapper and reducer jobs respectively.  The default seems to be 2 mapper jobs and 1 reducer job.  However, I can explicitly set the mapper jobs to 1 and the reducer jobs to 4 in the run command.  Perhaps in java though, the reducer tasks will run on each unique key by default, but this behavior does not appear to be the case in Hadoop Streaming.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2.0.2\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author:Jackson Lane\n",
    "# Description: mapper code for 3.2.0.2 and 3.2.0.3\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys,re\n",
    "sys.stderr.write(\"reporter:counter:3.2,MapperCount,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    #Get the issue field\n",
    "    issue = fields[3]\n",
    "    #split the issue field into individual words\n",
    "    words = re.findall(\"[\\w']+\",issue)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        # Even though we have two reducers, we still want to print out \n",
    "        # an alphabetically sorted list of word counts.  So we make sure\n",
    "        # to send every word that begins with a-l to the first partition\n",
    "        # and everything else to the second partition\n",
    "        partitionkey = int(word > \"m\")\n",
    "        #Emit each word with a count of 1\n",
    "        print (partitionkey,word,1,sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python \n",
    "# reducer.py\n",
    "# Author: Jackson Lane\n",
    "# Description: reducer code for 3.2.0.2, 3.2.0.3\n",
    "# Same as reducer code for 3.2.0.1, but adjusting for the extra partitionkey field\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "word = \"\"\n",
    "count = 0\n",
    "sys.stderr.write(\"reporter:counter:3.2,ReducerCount,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write(line)\n",
    "\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    _,newword, newcount = line.split(\",\")\n",
    "    newcount = int(newcount)\n",
    "    if (newword == word): count += newcount\n",
    "    else:\n",
    "        # We have finished with all instances of the current word.  \n",
    "        # Print total count and move on to next word\n",
    "        if (count > 0): print (word,count,sep=',')\n",
    "        word = newword\n",
    "        count = newcount\n",
    "if (count > 0): print (word,count,sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the map-reduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:02:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:02:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/3.2\n",
      "16/06/05 11:02:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:02:41 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 11:02:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /tmp/hadoop-unjar8007677370073082870/] [] /tmp/streamjob7713098128148077896.jar tmpDir=null\n",
      "16/06/05 11:02:42 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:02:42 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:02:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 11:02:42 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 11:02:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/06/05 11:02:42 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/06/05 11:02:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/05 11:02:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0187\n",
      "16/06/05 11:02:43 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0187\n",
      "16/06/05 11:02:43 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0187/\n",
      "16/06/05 11:02:43 INFO mapreduce.Job: Running job: job_1464324416493_0187\n",
      "16/06/05 11:02:50 INFO mapreduce.Job: Job job_1464324416493_0187 running in uber mode : false\n",
      "16/06/05 11:02:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 11:02:59 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/05 11:03:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 11:03:09 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/05 11:03:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 11:03:12 INFO mapreduce.Job: Job job_1464324416493_0187 completed successfully\n",
      "16/06/05 11:03:13 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13194441\n",
      "\t\tFILE: Number of bytes written=26879192\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910610\n",
      "\t\tHDFS: Number of bytes written=2091\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=57940\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=135736\n",
      "\t\tTotal time spent by all map tasks (ms)=14485\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16967\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14485\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16967\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=59330560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=138993664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=980482\n",
      "\t\tMap output bytes=11233465\n",
      "\t\tMap output materialized bytes=13194453\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=169\n",
      "\t\tReduce shuffle bytes=13194453\n",
      "\t\tReduce input records=980482\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=1960964\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=343\n",
      "\t\tCPU time spent (ms)=20990\n",
      "\t\tPhysical memory (bytes) snapshot=1130864640\n",
      "\t\tVirtual memory (bytes) snapshot=27566129152\n",
      "\t\tTotal committed heap usage (bytes)=1036517376\n",
      "\t3.2\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2091\n",
      "16/06/05 11:03:13 INFO streaming.StreamJob: Output directory: results/3.2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/3.2\n",
    "!hdfs dfs -put -p -f Consumer_Complaints.csv\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1n\" \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "-D mapreduce.output.key.field.separator=, \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.reduce.output.field.separator=, \\\n",
    "-D stream.map.input.field.separator=, \\\n",
    "-D stream.reduce.input.field.separator=, \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-file mapper.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output results/3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that the mapper and reducer counts are both 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:03:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t3503\n",
      "account\t20681\n",
      "acct\t163\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "an\t2505\n",
      "and\t16448\n",
      "application\t8868\n",
      "applied\t139\n",
      "apply\t118\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t11848\n",
      "available\t274\n",
      "balance\t597\n",
      "bank\t202\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cancelling\t2795\n",
      "card\t4405\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charged\t976\n",
      "charges\t131\n",
      "checks\t75\n",
      "closing\t2795\n",
      "club\t12545\n",
      "collect\t11848\n",
      "collection\t1907\n",
      "communication\t6920\n",
      "company's\t4858\n",
      "cont'd\t11848\n",
      "contact\t3053\n",
      "convenience\t75\n",
      "costs\t4350\n",
      "credit\t55251\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "debt\t19309\n",
      "decision\t2774\n",
      "decrease\t1149\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "did\t139\n",
      "didn't\t925\n",
      "disclosure\t5214\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "disputes\t6938\n",
      "embezzlement\t3276\n",
      "expect\t807\n",
      "false\t2508\n",
      "fee\t3198\n",
      "fees\t807\n",
      "for\t929\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "health\t12545\n",
      "i\t925\n",
      "identity\t4729\n",
      "illegal\t2505\n",
      "improper\t4309\n",
      "incorrect\t29133\n",
      "increase\t1149\n",
      "info\t2896\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t119630\n",
      "low\t5663\n",
      "making\t3226\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "missing\t64\n",
      "modification\t70487\n",
      "money\t413\n",
      "monitoring\t1453\n",
      "my\t10731\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "opening\t16205\n",
      "or\t22533\n",
      "other\t7886\n",
      "out\t1242\n",
      "overlimit\t127\n",
      "owed\t11848\n",
      "pay\t3821\n",
      "payment\t92\n",
      "payments\t3226\n",
      "payoff\t1155\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "rate\t3431\n",
      "receive\t139\n",
      "received\t216\n",
      "receiving\t3226\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "report\t34903\n",
      "reporting\t6559\n",
      "representation\t2508\n",
      "rewards\t1002\n",
      "sale\t139\n",
      "scam\t566\n",
      "score\t4357\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "servicing\t36767\n",
      "settlement\t4350\n",
      "sharing\t2832\n",
      "shopping\t672\n",
      "statement\t1220\n",
      "statements\t2508\n",
      "stop\t131\n",
      "tactics\t6920\n",
      "taking\t3747\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "threatening\t2505\n",
      "to\t8401\n",
      "transaction\t1485\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "underwriting\t2774\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "using\t2422\n",
      "verification\t5214\n",
      "was\t274\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.2/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2.0.3\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "# Use same mapper as in previous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python \n",
    "# combiner.py\n",
    "# Author: Jackson Lane\n",
    "# Description: combiner code for 3.2.0.3\n",
    "# Similar to reducer code from 3.2.0.2, except that the combiner \n",
    "# also prints the partition key and has a different counter\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "word = \"\"\n",
    "count = 0\n",
    "sys.stderr.write(\"reporter:counter:3.2,CombinerCount,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    partitionkey,newword, newcount = line.split(\",\")\n",
    "    newcount = int(newcount)\n",
    "    if (newword == word): count += newcount\n",
    "    else:\n",
    "        # We have finished with all instances of the current word.  \n",
    "        # Print total count and move on to next word\n",
    "        if (count > 0): print (partitionkey,word, count, sep=',')\n",
    "        word = newword\n",
    "        count = newcount\n",
    "if (count > 0): print (partitionkey,word,  count, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use same reducer as in previous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run map reduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:04:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:04:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/3.2\n",
      "16/06/05 11:04:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:04:33 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 11:04:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, combiner.py, /tmp/hadoop-unjar5076983616027755871/] [] /tmp/streamjob6710361804620623018.jar tmpDir=null\n",
      "16/06/05 11:04:34 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:04:34 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:04:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 11:04:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 11:04:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0188\n",
      "16/06/05 11:04:36 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0188\n",
      "16/06/05 11:04:36 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0188/\n",
      "16/06/05 11:04:36 INFO mapreduce.Job: Running job: job_1464324416493_0188\n",
      "16/06/05 11:04:42 INFO mapreduce.Job: Job job_1464324416493_0188 running in uber mode : false\n",
      "16/06/05 11:04:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 11:04:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 11:04:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 11:04:57 INFO mapreduce.Job: Job job_1464324416493_0188 completed successfully\n",
      "16/06/05 11:04:57 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5064\n",
      "\t\tFILE: Number of bytes written=379727\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910610\n",
      "\t\tHDFS: Number of bytes written=3739\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=64184\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25184\n",
      "\t\tTotal time spent by all map tasks (ms)=16046\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3148\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16046\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3148\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=65724416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=25788416\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=980482\n",
      "\t\tMap output bytes=11233465\n",
      "\t\tMap output materialized bytes=5070\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=980482\n",
      "\t\tCombine output records=313\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce shuffle bytes=5070\n",
      "\t\tReduce input records=313\n",
      "\t\tReduce output records=307\n",
      "\t\tSpilled Records=626\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=224\n",
      "\t\tCPU time spent (ms)=8220\n",
      "\t\tPhysical memory (bytes) snapshot=859910144\n",
      "\t\tVirtual memory (bytes) snapshot=19001569280\n",
      "\t\tTotal committed heap usage (bytes)=740294656\n",
      "\t3.2\n",
      "\t\tCombinerCount=2\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3739\n",
      "16/06/05 11:04:57 INFO streaming.StreamJob: Output directory: results/3.2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/3.2\n",
    "!hdfs dfs -put -p -f Consumer_Complaints.csv\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1n\" \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "-D mapreduce.output.key.field.separator=, \\\n",
    "-D mapreduce.map.output.key.field.separator=, \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.reduce.output.field.separator=, \\\n",
    "-D stream.map.input.field.separator=, \\\n",
    "-D stream.reduce.input.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-file mapper.py \\\n",
    "-file reducer.py \\\n",
    "-file combiner.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-combiner \"combiner.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output results/3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:05:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t3503\n",
      "account\t20681\n",
      "acct\t163\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "an\t229\n",
      "amt\t71\n",
      "and\t10227\n",
      "an\t2276\n",
      "application\t5962\n",
      "and\t6221\n",
      "apr\t2557\n",
      "application\t2906\n",
      "arbitration\t92\n",
      "applied\t139\n",
      "are\t2510\n",
      "apply\t118\n",
      "atm\t1366\n",
      "apr\t874\n",
      "attempts\t1372\n",
      "arbitration\t76\n",
      "available\t64\n",
      "are\t1311\n",
      "balance\t400\n",
      "atm\t1056\n",
      "bankruptcy\t143\n",
      "attempts\t10476\n",
      "being\t3743\n",
      "available\t210\n",
      "billing\t5289\n",
      "balance\t197\n",
      "by\t3743\n",
      "bank\t202\n",
      "cancelling\t1822\n",
      "bankruptcy\t79\n",
      "card\t2673\n",
      "being\t1920\n",
      "cash\t167\n",
      "billing\t2869\n",
      "caused\t3743\n",
      "by\t1920\n",
      "changes\t235\n",
      "can't\t1999\n",
      "charged\t17\n",
      "cancelling\t973\n",
      "checks\t49\n",
      "card\t1732\n",
      "closing\t1822\n",
      "cash\t73\n",
      "club\t1783\n",
      "caused\t1920\n",
      "collect\t1372\n",
      "changes\t115\n",
      "collection\t1907\n",
      "charged\t959\n",
      "communication\t795\n",
      "charges\t131\n",
      "company's\t1913\n",
      "checks\t26\n",
      "cont'd\t1372\n",
      "closing\t973\n",
      "contact\t282\n",
      "club\t10762\n",
      "convenience\t49\n",
      "collect\t10476\n",
      "costs\t2811\n",
      "communication\t6125\n",
      "credit\t21686\n",
      "company's\t2945\n",
      "customer\t1532\n",
      "cont'd\t10476\n",
      "debit\t1366\n",
      "contact\t2771\n",
      "debt\t3920\n",
      "convenience\t26\n",
      "decision\t1823\n",
      "costs\t1539\n",
      "decrease\t826\n",
      "credit\t33565\n",
      "delay\t170\n",
      "credited\t92\n",
      "delinquent\t460\n",
      "customer\t1202\n",
      "deposits\t6488\n",
      "day\t71\n",
      "determination\t960\n",
      "dealing\t1944\n",
      "disclosure\t697\n",
      "debit\t1056\n",
      "disclosures\t14\n",
      "debt\t15389\n",
      "dispute\t904\n",
      "decision\t951\n",
      "disputes\t4528\n",
      "decrease\t323\n",
      "embezzlement\t1929\n",
      "delay\t73\n",
      "false\t275\n",
      "delinquent\t601\n",
      "fee\t2080\n",
      "deposits\t4067\n",
      "for\t287\n",
      "determination\t530\n",
      "forbearance\t256\n",
      "did\t139\n",
      "fraud\t2052\n",
      "didn't\t925\n",
      "funds\t3743\n",
      "disclosure\t4517\n",
      "get\t1489\n",
      "disclosures\t50\n",
      "getting\t193\n",
      "disputes\t2410\n",
      "health\t1783\n",
      "embezzlement\t1347\n",
      "identity\t2390\n",
      "expect\t807\n",
      "illegal\t229\n",
      "false\t2233\n",
      "improper\t940\n",
      "fee\t1118\n",
      "incorrect\t8711\n",
      "fees\t807\n",
      "increase\t826\n",
      "for\t642\n",
      "info\t296\n",
      "forbearance\t94\n",
      "information\t8697\n",
      "fraud\t1790\n",
      "interest\t2557\n",
      "funds\t1920\n",
      "investigation\t1913\n",
      "get\t2868\n",
      "issuance\t360\n",
      "getting\t98\n",
      "issue\t636\n",
      "health\t10762\n",
      "issues\t139\n",
      "i\t925\n",
      "late\t1142\n",
      "identity\t2339\n",
      "lease\t2909\n",
      "illegal\t2276\n",
      "line\t1188\n",
      "improper\t3369\n",
      "loan\t76245\n",
      "incorrect\t20422\n",
      "low\t3743\n",
      "increase\t323\n",
      "making\t1964\n",
      "info\t2600\n",
      "managing\t2485\n",
      "information\t20372\n",
      "marketing\t693\n",
      "interest\t1681\n",
      "missing\t14\n",
      "investigation\t2945\n",
      "modification\t48670\n",
      "issuance\t280\n",
      "money\t64\n",
      "issue\t462\n",
      "monitoring\t461\n",
      "issues\t399\n",
      "my\t4401\n",
      "late\t655\n",
      "not\t1436\n",
      "lease\t3428\n",
      "of\t2438\n",
      "lender\t2165\n",
      "on\t8697\n",
      "line\t544\n",
      "opening\t9567\n",
      "loan\t43385\n",
      "or\t8004\n",
      "low\t1920\n",
      "other\t4654\n",
      "making\t1262\n",
      "out\t499\n",
      "managing\t2521\n",
      "overlimit\t84\n",
      "marketing\t500\n",
      "owed\t1372\n",
      "missing\t50\n",
      "pay\t2510\n",
      "modification\t21817\n",
      "payments\t1964\n",
      "money\t349\n",
      "payoff\t802\n",
      "monitoring\t992\n",
      "plans\t256\n",
      "my\t6330\n",
      "practices\t1003\n",
      "not\t10917\n",
      "privacy\t141\n",
      "of\t8447\n",
      "problems\t6253\n",
      "on\t20372\n",
      "process\t3613\n",
      "opening\t6638\n",
      "processing\t170\n",
      "or\t14529\n",
      "promised\t64\n",
      "other\t3232\n",
      "protection\t2355\n",
      "out\t743\n",
      "rate\t2557\n",
      "overlimit\t43\n",
      "received\t17\n",
      "owed\t10476\n",
      "receiving\t1964\n",
      "pay\t1311\n",
      "relations\t766\n",
      "payment\t92\n",
      "repaying\t3409\n",
      "payments\t1262\n",
      "report\t10844\n",
      "payoff\t353\n",
      "reporting\t3614\n",
      "plans\t94\n",
      "representation\t275\n",
      "privacy\t99\n",
      "rewards\t671\n",
      "problems\t3231\n",
      "sale\t79\n",
      "process\t1892\n",
      "scam\t123\n",
      "processing\t73\n",
      "score\t1489\n",
      "promised\t210\n",
      "service\t794\n",
      "protection\t1784\n",
      "servicing\t21064\n",
      "rate\t874\n",
      "settlement\t2811\n",
      "receive\t139\n",
      "sharing\t282\n",
      "received\t199\n",
      "shopping\t287\n",
      "receiving\t1262\n",
      "statement\t761\n",
      "relations\t601\n",
      "statements\t275\n",
      "repay\t1647\n",
      "tactics\t795\n",
      "repaying\t435\n",
      "taking\t728\n",
      "report\t24059\n",
      "terms\t235\n",
      "reporting\t2945\n",
      "the\t2984\n",
      "representation\t2233\n",
      "theft\t1929\n",
      "rewards\t331\n",
      "threatening\t229\n",
      "sale\t60\n",
      "to\t3999\n",
      "scam\t443\n",
      "transaction\t747\n",
      "score\t2868\n",
      "transfer\t400\n",
      "service\t724\n",
      "unable\t3999\n",
      "servicer\t1944\n",
      "underwriting\t1823\n",
      "servicing\t15703\n",
      "unsolicited\t360\n",
      "settlement\t1539\n",
      "use\t658\n",
      "sharing\t2550\n",
      "using\t1366\n",
      "shopping\t385\n",
      "verification\t697\n",
      "statement\t459\n",
      "was\t64\n",
      "statements\t2233\n",
      "when\t2574\n",
      "stop\t131\n",
      "withdrawals\t6488\n",
      "tactics\t6125\n",
      "workout\t256\n",
      "taking\t3019\n",
      "wrong\t17\n",
      "terms\t115\n",
      "you\t2510\n",
      "the\t3264\n",
      "your\t3409\n",
      "theft\t1347\n",
      "threatening\t2276\n",
      "to\t4402\n",
      "transaction\t738\n",
      "transfer\t197\n",
      "unable\t4179\n",
      "underwriting\t951\n",
      "unsolicited\t280\n",
      "use\t819\n",
      "using\t1056\n",
      "verification\t4517\n",
      "was\t210\n",
      "when\t1521\n",
      "with\t1944\n",
      "withdrawals\t4067\n",
      "workout\t94\n",
      "wrong\t152\n",
      "you\t1311\n",
      "your\t435\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.2/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The mapper and reducer counts were 2 and 1 respectively, which are the defaults for Hadoop Streaming.  The combiner counter was 2.  This is somewhat strange as there were certainly more than 2 different unique keys emitted by the mappers.  However, Hadoop does not guarantee that the combiner will run a certain number of times. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2.0.4\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author:Jackson Lane\n",
    "# Description: mapper code for HW3.2.0.4\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys,re\n",
    "sys.stderr.write(\"reporter:counter:3.2,MapperCount,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    issue = fields[3]\n",
    "    words = re.findall(\"[\\w']+\",issue)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        # I'm emitting the number first here so that I can use order inversion to emit the total\n",
    "        print (word,1,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# reducer.py\n",
    "# Author:Jackson Lane\n",
    "# Description: reducer code for HW3.2.0.4\n",
    "# Since we are using a single reducer, it has to sum up the word counts and get the top 50 and bottom 10.\n",
    "\n",
    "\n",
    "import sys,Queue\n",
    "from collections import deque\n",
    "words = {}\n",
    "sys.stderr.write(\"reporter:counter:3.2,ReducerCount,1\\n\")\n",
    "total = float(0)\n",
    "min_elements= []\n",
    "max_elements = deque([])\n",
    "for line in sys.stdin:\n",
    "        #Parse in the word and count from the mapper\n",
    "        word,value = line.split(\",\")\n",
    "        word = word\n",
    "        value = int(value)\n",
    "        #Update the master word count dictionary\n",
    "        words[word] = words.setdefault(word, 0) + value\n",
    "        #Update the total number of words as well\n",
    "        total += value\n",
    "#Convert the words dictionary into an array of tuples\n",
    "words =[(k, v) for k, v in words.iteritems()]\n",
    "#Sort the array of tuples by count and then alphabetically by word.\n",
    "words   = sorted(words,key=lambda x: x[0])\n",
    "words   = sorted(words,key=lambda x: x[1])\n",
    "\n",
    "#Get the top 50 and bottom elements from the list of tuples\n",
    "for (word,value) in words:\n",
    "        value = float(value)\n",
    "        if len(min_elements) < 10:\n",
    "            min_elements.append((word,value,value/total))\n",
    "        if(len(max_elements) >= 50):\n",
    "            max_elements.popleft()\n",
    "        max_elements.append((word,value,value/total))\n",
    "\n",
    "#Print out the max 50 and min 10 elements\n",
    "print \"50 most common tokens:\"\n",
    "for i,p in enumerate(max_elements):\n",
    "        print str(50-i)+\":\",p\n",
    "print \"10 least common tokens:\"\n",
    "for i,p in enumerate(min_elements):\n",
    "        print str(i+1)+\":\",p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:11:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:11:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/3.2\n",
      "16/06/05 11:11:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `Consumer_Complaints.csv': File exists\n",
      "16/06/05 11:11:49 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 11:11:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /tmp/hadoop-unjar3310236814296116326/] [] /tmp/streamjob6610809708114615817.jar tmpDir=null\n",
      "16/06/05 11:11:50 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:11:50 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:11:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 11:11:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 11:11:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/06/05 11:11:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0189\n",
      "16/06/05 11:11:52 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0189\n",
      "16/06/05 11:11:52 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0189/\n",
      "16/06/05 11:11:52 INFO mapreduce.Job: Running job: job_1464324416493_0189\n",
      "16/06/05 11:11:59 INFO mapreduce.Job: Job job_1464324416493_0189 running in uber mode : false\n",
      "16/06/05 11:11:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 11:12:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 11:12:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 11:12:17 INFO mapreduce.Job: Job job_1464324416493_0189 completed successfully\n",
      "16/06/05 11:12:17 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12213953\n",
      "\t\tFILE: Number of bytes written=24791994\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910610\n",
      "\t\tHDFS: Number of bytes written=2833\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=48336\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=57896\n",
      "\t\tTotal time spent by all map tasks (ms)=12084\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7237\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12084\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7237\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=49496064\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=59285504\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=980482\n",
      "\t\tMap output bytes=10252983\n",
      "\t\tMap output materialized bytes=12213959\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=593442\n",
      "\t\tReduce shuffle bytes=12213959\n",
      "\t\tReduce input records=980482\n",
      "\t\tReduce output records=62\n",
      "\t\tSpilled Records=1960964\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=196\n",
      "\t\tCPU time spent (ms)=9020\n",
      "\t\tPhysical memory (bytes) snapshot=780902400\n",
      "\t\tVirtual memory (bytes) snapshot=19000471552\n",
      "\t\tTotal committed heap usage (bytes)=644874240\n",
      "\t3.2\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2833\n",
      "16/06/05 11:12:17 INFO streaming.StreamJob: Output directory: results/3.2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/3.2\n",
    "!hdfs dfs -put Consumer_Complaints.csv \n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=-n \\\n",
    "-D mapreduce.output.key.field.separator=\",\" \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output results/3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:12:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "50 most common tokens:\t\n",
      "50: ('score', 4357.0, 0.004443732776328377)\t\n",
      "49: ('card', 4405.0, 0.004492688290045101)\t\n",
      "48: ('identity', 4729.0, 0.0048231380076329804)\t\n",
      "47: (\"company's\", 4858.0, 0.0049547059507466734)\t\n",
      "46: ('investigation', 4858.0, 0.0049547059507466734)\t\n",
      "45: ('managing', 5006.0, 0.005105652118039903)\t\n",
      "44: ('disclosure', 5214.0, 0.0053177926774790355)\t\n",
      "43: ('verification', 5214.0, 0.0053177926774790355)\t\n",
      "42: ('process', 5505.0, 0.005614585479386669)\t\n",
      "41: ('being', 5663.0, 0.0057757307120375485)\t\n",
      "40: ('by', 5663.0, 0.0057757307120375485)\t\n",
      "39: ('caused', 5663.0, 0.0057757307120375485)\t\n",
      "38: ('funds', 5663.0, 0.0057757307120375485)\t\n",
      "37: ('low', 5663.0, 0.0057757307120375485)\t\n",
      "36: ('the', 6248.0, 0.0063723760354601105)\t\n",
      "35: ('lease', 6337.0, 0.006463147717143201)\t\n",
      "34: ('reporting', 6559.0, 0.006689566968083045)\t\n",
      "33: ('communication', 6920.0, 0.007057753227494233)\t\n",
      "32: ('tactics', 6920.0, 0.007057753227494233)\t\n",
      "31: ('disputes', 6938.0, 0.007076111545138004)\t\n",
      "30: ('other', 7886.0, 0.008042982941043282)\t\n",
      "29: ('billing', 8158.0, 0.00832039751877138)\t\n",
      "28: ('unable', 8178.0, 0.008340795649486681)\t\n",
      "27: ('to', 8401.0, 0.00856823480696229)\t\n",
      "26: ('application', 8868.0, 0.009044531159164574)\t\n",
      "25: ('problems', 9484.0, 0.009672793585195853)\t\n",
      "24: ('deposits', 10555.0, 0.010765113485000234)\t\n",
      "23: ('withdrawals', 10555.0, 0.010765113485000234)\t\n",
      "22: ('my', 10731.0, 0.010944617035294885)\t\n",
      "21: ('of', 10885.0, 0.011101682641802705)\t\n",
      "20: ('attempts', 11848.0, 0.01208385263574446)\t\n",
      "19: ('collect', 11848.0, 0.01208385263574446)\t\n",
      "18: (\"cont'd\", 11848.0, 0.01208385263574446)\t\n",
      "17: ('owed', 11848.0, 0.01208385263574446)\t\n",
      "16: ('not', 12353.0, 0.012598905436305817)\t\n",
      "15: ('club', 12545.0, 0.012794727491172709)\t\n",
      "14: ('health', 12545.0, 0.012794727491172709)\t\n",
      "13: ('opening', 16205.0, 0.01652758541207284)\t\n",
      "12: ('and', 16448.0, 0.016775422700263748)\t\n",
      "11: ('debt', 19309.0, 0.01969337529908759)\t\n",
      "10: ('account', 20681.0, 0.02109268706615726)\t\n",
      "9: ('or', 22533.0, 0.022981553970394152)\t\n",
      "8: ('information', 29069.0, 0.029647663088154603)\t\n",
      "7: ('on', 29069.0, 0.029647663088154603)\t\n",
      "6: ('incorrect', 29133.0, 0.029712937106443564)\t\n",
      "5: ('report', 34903.0, 0.035597797817807975)\t\n",
      "4: ('servicing', 36767.0, 0.03749890360047405)\t\n",
      "3: ('credit', 55251.0, 0.05635085600755547)\t\n",
      "2: ('modification', 70487.0, 0.07189015198647196)\t\n",
      "1: ('loan', 119630.0, 0.12201141887357443)\t\n",
      "10 least common tokens:\t\n",
      "1: ('disclosures', 64.0, 6.5274018288964e-05)\t\n",
      "2: ('missing', 64.0, 6.5274018288964e-05)\t\n",
      "3: ('amt', 71.0, 7.241336403931944e-05)\t\n",
      "4: ('day', 71.0, 7.241336403931944e-05)\t\n",
      "5: ('checks', 75.0, 7.649299018237969e-05)\t\n",
      "6: ('convenience', 75.0, 7.649299018237969e-05)\t\n",
      "7: ('credited', 92.0, 9.383140129038574e-05)\t\n",
      "8: ('payment', 92.0, 9.383140129038574e-05)\t\n",
      "9: ('amount', 98.0, 9.995084050497613e-05)\t\n",
      "10: ('apply', 118.0, 0.00012034897122027737)\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2.1  \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> For this problem, my understanding from the discussion board was to use a single mapreduce job with a single reducer.py with two reducer tasks.  I understand that other students interpreted the problem as meaning they could use two different reducer.py files across two different map reduce jobs.  I also understand that other students interpreted the problem as meaning they could feed in the wordcounts from the previous parts of 3.2 into this problem.  Both of these methods might have been easier, but I wanted to see if I could perform the word count and get the top 50 and bottom 10 elements with just a single map reduce job starting from the Consumer_Complaints.csv.  \n",
    "\n",
    "The challenging part was getting the top 50 and bottom 10 elements into different partitions while still keeping in the spirit of MapReduce.  There's not enough information at the mapper stage to determine whether a word is in the top 50 or bottom 10.  One could put some reducer functionality in the mapper to first aggregate the counts of each word and then determine the partition key, but that goes against the intended purpose of a mapper.\n",
    "\n",
    "To work around this, I had my mappers emit each word twice (once for each partition).  Normally, this would defeat the purpose of having partitions as each reducer would need to process the full dataset.  However, I also wrote my combiners to filter the mapper outputs after aggregation so that only the high count words make it to the top 50 reducer task and only the low count words make it to the bottom 10 reducer task.  This type of solution also fits with the combiners expressed purpose of reducing the volume of data sent to the reducers.\n",
    "\n",
    "While my solution gets the top 50 and bottom 10 words, the accurarcy drops beyond those ranks (for example, it's not accurate on the top 100 and bottom 50 words).  Since a combiner may be applied on just a subset of the keys emitted by the mapper, the combiner cannot know for sure whether a word appears with a high or low frequency relative to the other words in the corpus.  In fact, the combiner cannot even know for sure the total number of words in the corpus as it might not be processing all the words.  The reducer does not have this problem because it has barrier synchronization. </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author:Jackson Lane\n",
    "# Description: mapper code for HW3.2.1\n",
    "from __future__ import print_function\n",
    "from sets import Set\n",
    "import sys,re,random\n",
    "sys.stderr.write(\"reporter:counter:3.2,MapperCount,1\\n\")\n",
    "total = 0\n",
    "\n",
    "uniquewords = Set([])\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    issue = fields[3]\n",
    "    words = re.findall(\"[\\w']+\",issue)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        uniquewords.add(word)\n",
    "        total+= 1\n",
    "        # I'm emitting the number first here so that I can use order inversion to emit the total\n",
    "        print(0,1,word,sep=\"\\t\")\n",
    "        print(1,1,word,sep=\"\\t\")\n",
    "\n",
    "\n",
    "#Since we need to compute relative frequencies in the reducer, we need the total number of issues\n",
    "#But since reading counters from within a job is apparently bad practice, I'm supposed to print\n",
    "# a special key value pair with the fields that I need.\n",
    "# emit total twice for each partition\n",
    "print(0,0,total,sep=\"\\t\")\n",
    "print(0,-1,uniquewords,sep=\"\\t\")\n",
    "print(1,0,total,sep=\"\\t\")\n",
    "print(1,-1,uniquewords,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python \n",
    "# reducer.py\n",
    "# Author: Jackson Lane\n",
    "# Description: combiner code for 3.2.1\n",
    "# Acts like router, only sending the small word counts to the first reducer and the large word counts to the second red\n",
    "# This type of \"filter combiner\" currently only works with 2 mappers\n",
    "# Alternatively I could just remove the filter condition and make it so that this combiner passes through all the\n",
    "# word counts.  That would be scalable to any number of mappers, but it would defeat the purpose of parallelism\n",
    "# as each reducer would process the full word count work load.  \n",
    "# If there are more mappers, then this combiner will end up sending the wrong lines to the wrong partitions\n",
    "\n",
    "from __future__ import print_function\n",
    "from sets import Set\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "#Output to combiner is sorted and paritioned already, so we can use reducer word count log\n",
    "partition = 0\n",
    "word = \"\"\n",
    "count = 0\n",
    "#Since each instance of a combiner only processes a subset of the data emitted by the mappers, \n",
    "# the distribution of the words observed in the combiner is not always representative of\n",
    "# the distribution of words in the rest of the corpus.\n",
    "# But since the we want the top 50 words but only the bottom 10 words, we can err on the side\n",
    "# of the top 50 since it needs a higher level of accuracy.  The below magic number means that\n",
    "# we send every word with a count that is in the top 80% to the top 50 reducer and the bottom\n",
    "# 20% to the bottom 10 reducer\n",
    "magicnumber = .4\n",
    "sys.stderr.write(\"reporter:counter:3.2,CombinerCount,1\\n\")\n",
    "total = float(0)\n",
    "sys.stderr.write(\"combiner\\n\")\n",
    "uniquewords = Set([])\n",
    "\n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write(line)\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    newpartition, newcount, newword = line.split(\"\\t\")\n",
    "    newcount = int(newcount)\n",
    "    newpartition = int(newpartition)\n",
    "    # Update the summary statistic variable we passed along from the mapper\n",
    "    # These values should appear first in the stdin because they have counts\n",
    "    # of 0 and -1\n",
    "    if newcount == 0: \n",
    "        total += float(newword)\n",
    "        continue\n",
    "    if newcount == -1: \n",
    "        uniquewords.update(eval(newword))\n",
    "        continue\n",
    "        \n",
    "    #Update the word counts\n",
    "    if (newword == word and partition == newpartition):\n",
    "        count += newcount\n",
    "    else:\n",
    "        # We have finished with all instances of the current word.  \n",
    "        # Now determine which partition the word should go to\n",
    "        partitionkey =int(count> total*magicnumber / len(uniquewords))\n",
    "\n",
    "        # If word is routed to correct partition, emit word and count.  Otherwise, just skip word\n",
    "        if (count > 0 and partitionkey == partition): print(partition,count,word, sep='\\t')\n",
    "        word = newword\n",
    "        count = newcount\n",
    "        partition = newpartition\n",
    "\n",
    "#Emit last word count if going to right partition\n",
    "partitionkey =int(count > total*magicnumber  / len(uniquewords))\n",
    "if (count > 0 and partitionkey == partition): \n",
    "    print(partition,count,word, sep='\\t')\n",
    "\n",
    "#pass on totals again\n",
    "print(partitionkey,0,3total,sep=\"\\t\")\n",
    "#We don't need to pass on the uniquewordcount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# reducer.py\n",
    "# Author:Jackson Lane\n",
    "# Description: reducer code for HW3.2.1\n",
    "# This reducer collects all the incoming words into a dictionary\n",
    "# with their counts and relative frequencies.  Then the reducer\n",
    "# sorts the dictionary by count and outputs either the\n",
    "# top 50 or bottom 10 word counts\n",
    "\n",
    "import sys\n",
    "words = {}\n",
    "sys.stderr.write(\"reporter:counter:3.2,ReducerCount,1\\n\")\n",
    "total = float(0)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    p, count, word = line.split(\"\\t\")\n",
    "    p = int(p)\n",
    "    count = float(count)\n",
    "    # Update the total summary statistic variable\n",
    "    # Since combiner output is not sorted this one may\n",
    "    # no longer be at the top of the stdin.  So\n",
    "    # that's why we can only compute relative frequency\n",
    "    # after all the stdin has been processed.\n",
    "    if count == 0: \n",
    "        total += float(word)\n",
    "        continue\n",
    "\n",
    "    #Update the word count dictionary. \n",
    "    words[word] = words.setdefault(word, 0) + count\n",
    "\n",
    "#Get which partition this reducer represents. \n",
    "# If 0, then it's the bottom 10 reducer.\n",
    "# If 1, then it's the top 50 reducer.\n",
    "partition = p\n",
    "\n",
    "#Turn dictionary into sorted array of tuples with counts and \n",
    "# relative frequencies\n",
    "words =[(k, v,float(v)/total) for k, v in words.iteritems()]\n",
    "words   = sorted(words,key=lambda x: x[0])\n",
    "words   = sorted(words,key=lambda x: x[1])\n",
    "\n",
    "#Print either top 50 or bottom 10\n",
    "if partition == 1:\n",
    "    print \"50 most common tokens:\"\n",
    "    for i,p in enumerate(words[-50:]):\n",
    "        print str(50-i)+\":\",p\n",
    "    \n",
    "else:\n",
    "    print \"10 least common tokens:\"\n",
    "    for i,p in enumerate(words[:10]):\n",
    "        print str(i+1)+\":\",p\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:15:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:15:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/3.2\n",
      "16/06/05 11:15:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `Consumer_Complaints.csv': File exists\n",
      "16/06/05 11:16:00 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 11:16:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /tmp/hadoop-unjar1441934325260427106/] [] /tmp/streamjob1667956411416720403.jar tmpDir=null\n",
      "16/06/05 11:16:01 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:16:01 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:16:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 11:16:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 11:16:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0191\n",
      "16/06/05 11:16:03 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0191\n",
      "16/06/05 11:16:03 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0191/\n",
      "16/06/05 11:16:03 INFO mapreduce.Job: Running job: job_1464324416493_0191\n",
      "16/06/05 11:16:10 INFO mapreduce.Job: Job job_1464324416493_0191 running in uber mode : false\n",
      "16/06/05 11:16:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 11:16:21 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/05 11:16:29 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/06/05 11:16:30 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 11:16:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 11:16:36 INFO mapreduce.Job: Job job_1464324416493_0191 completed successfully\n",
      "16/06/05 11:16:36 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5130\n",
      "\t\tFILE: Number of bytes written=499622\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910610\n",
      "\t\tHDFS: Number of bytes written=2831\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=140068\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=47752\n",
      "\t\tTotal time spent by all map tasks (ms)=35017\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5969\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=35017\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5969\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=143429632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=48898048\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=1960972\n",
      "\t\tMap output bytes=24434674\n",
      "\t\tMap output materialized bytes=5142\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=1960972\n",
      "\t\tCombine output records=317\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=5142\n",
      "\t\tReduce input records=317\n",
      "\t\tReduce output records=62\n",
      "\t\tSpilled Records=634\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=369\n",
      "\t\tCPU time spent (ms)=33610\n",
      "\t\tPhysical memory (bytes) snapshot=1195876352\n",
      "\t\tVirtual memory (bytes) snapshot=27568566272\n",
      "\t\tTotal committed heap usage (bytes)=1041760256\n",
      "\t3.2\n",
      "\t\tCombinerCount=4\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910408\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2831\n",
      "16/06/05 11:16:36 INFO streaming.StreamJob: Output directory: results/3.2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/3.2\n",
    "!hdfs dfs -put Consumer_Complaints.csv \n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2n -k3,3\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1n\" \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapreduce.job.reduces=2 \\\n",
    "-file mapper.py \\\n",
    "-file combiner.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-combiner \"combiner.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output results/3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:17:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "10 least common tokens:\t\n",
      "1: ('disclosures', 64.0, 6.5274018288964e-05)\t\n",
      "2: ('missing', 64.0, 6.5274018288964e-05)\t\n",
      "3: ('amt', 71.0, 7.241336403931944e-05)\t\n",
      "4: ('day', 71.0, 7.241336403931944e-05)\t\n",
      "5: ('checks', 75.0, 7.649299018237969e-05)\t\n",
      "6: ('convenience', 75.0, 7.649299018237969e-05)\t\n",
      "7: ('credited', 92.0, 9.383140129038574e-05)\t\n",
      "8: ('payment', 92.0, 9.383140129038574e-05)\t\n",
      "9: ('amount', 98.0, 9.995084050497613e-05)\t\n",
      "10: ('apply', 118.0, 0.00012034897122027737)\t\n",
      "50 most common tokens:\t\n",
      "50: ('score', 4357.0, 0.004443732776328377)\t\n",
      "49: ('card', 4405.0, 0.004492688290045101)\t\n",
      "48: ('disclosure', 4517.0, 0.004606917822050787)\t\n",
      "47: ('verification', 4517.0, 0.004606917822050787)\t\n",
      "46: ('identity', 4729.0, 0.0048231380076329804)\t\n",
      "45: (\"company's\", 4858.0, 0.0049547059507466734)\t\n",
      "44: ('investigation', 4858.0, 0.0049547059507466734)\t\n",
      "43: ('managing', 5006.0, 0.005105652118039903)\t\n",
      "42: ('process', 5505.0, 0.005614585479386669)\t\n",
      "41: ('being', 5663.0, 0.0057757307120375485)\t\n",
      "40: ('by', 5663.0, 0.0057757307120375485)\t\n",
      "39: ('caused', 5663.0, 0.0057757307120375485)\t\n",
      "38: ('funds', 5663.0, 0.0057757307120375485)\t\n",
      "37: ('low', 5663.0, 0.0057757307120375485)\t\n",
      "36: ('communication', 6125.0, 0.006246927531561008)\t\n",
      "35: ('tactics', 6125.0, 0.006246927531561008)\t\n",
      "34: ('the', 6248.0, 0.0063723760354601105)\t\n",
      "33: ('lease', 6337.0, 0.006463147717143201)\t\n",
      "32: ('reporting', 6559.0, 0.006689566968083045)\t\n",
      "31: ('disputes', 6938.0, 0.007076111545138004)\t\n",
      "30: ('other', 7886.0, 0.008042982941043282)\t\n",
      "29: ('billing', 8158.0, 0.00832039751877138)\t\n",
      "28: ('unable', 8178.0, 0.008340795649486681)\t\n",
      "27: ('to', 8401.0, 0.00856823480696229)\t\n",
      "26: ('application', 8868.0, 0.009044531159164574)\t\n",
      "25: ('problems', 9484.0, 0.009672793585195853)\t\n",
      "24: ('deposits', 10555.0, 0.010765113485000234)\t\n",
      "23: ('withdrawals', 10555.0, 0.010765113485000234)\t\n",
      "22: ('my', 10731.0, 0.010944617035294885)\t\n",
      "21: ('of', 10885.0, 0.011101682641802705)\t\n",
      "20: ('attempts', 11848.0, 0.01208385263574446)\t\n",
      "19: ('collect', 11848.0, 0.01208385263574446)\t\n",
      "18: (\"cont'd\", 11848.0, 0.01208385263574446)\t\n",
      "17: ('owed', 11848.0, 0.01208385263574446)\t\n",
      "16: ('not', 12353.0, 0.012598905436305817)\t\n",
      "15: ('club', 12545.0, 0.012794727491172709)\t\n",
      "14: ('health', 12545.0, 0.012794727491172709)\t\n",
      "13: ('opening', 16205.0, 0.01652758541207284)\t\n",
      "12: ('and', 16448.0, 0.016775422700263748)\t\n",
      "11: ('debt', 19309.0, 0.01969337529908759)\t\n",
      "10: ('account', 20681.0, 0.02109268706615726)\t\n",
      "9: ('or', 22533.0, 0.022981553970394152)\t\n",
      "8: ('information', 29069.0, 0.029647663088154603)\t\n",
      "7: ('on', 29069.0, 0.029647663088154603)\t\n",
      "6: ('incorrect', 29133.0, 0.029712937106443564)\t\n",
      "5: ('report', 34903.0, 0.035597797817807975)\t\n",
      "4: ('servicing', 36767.0, 0.03749890360047405)\t\n",
      "3: ('credit', 55251.0, 0.05635085600755547)\t\n",
      "2: ('modification', 70487.0, 0.07189015198647196)\t\n",
      "1: ('loan', 119630.0, 0.12201141887357443)\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.2/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author:Jackson Lane\n",
    "# Description: mapper code for HW3.3\n",
    "from __future__ import print_function\n",
    "from sets import Set\n",
    "import sys,re\n",
    "sys.stderr.write(\"reporter:counter:3.3,MapperCount,1\\n\")\n",
    "total = 0\n",
    "#Maintain a set of unique products to pass to reducer\n",
    "uniqueproducts = Set([])\n",
    "for line in sys.stdin:\n",
    "    basket = re.findall(\"[\\w']+\",line)\n",
    "    for product in basket:\n",
    "        uniqueproducts.add(product)\n",
    "        total+= 1\n",
    "        #Emit three fields for the reducer to use:\n",
    "        # Product, count, and basket size\n",
    "        # Basket size will be emitted once for each\n",
    "        # product in the basket.  This is inefficient\n",
    "        # from a network bandwidth perspective, but\n",
    "        # relatively harmless as it's just an \n",
    "        # extra integer.\n",
    "        # If we really wanted to save bandwidth, we \n",
    "        # could just emit the product by itself and\n",
    "        # nothing else.  The reducer could assume that\n",
    "        # the count will be 1.\n",
    "        print (product,1,len(basket),sep=\"\\t\")\n",
    "\n",
    "\n",
    "#Emit the total number of products and the set of unique products to reducer\n",
    "print('Totals',total,uniqueproducts,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# reducer.py\n",
    "# Author:Jackson Lane\n",
    "# Description: reducer code for HW3.3\n",
    "# This reducer gets the products from the mapper and computes\n",
    "# The number of unique products, the largest basket, and the\n",
    "# top 50 products.\n",
    "# Since we are using a single reducer, we have more flexibility\n",
    "# with what we are allowed to do.\n",
    "\n",
    "import sys\n",
    "from sets import Set\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:3.3,ReducerCount,1\\n\")\n",
    "total = float(0)\n",
    "uniqueproducts = Set([])\n",
    "\n",
    "# These fields are similar to the fields from the word count\n",
    "# code, but renamed for products instead of words\n",
    "product = 0\n",
    "maxbasketsize = 0\n",
    "count =0 \n",
    "products = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #Get the product, count, and basket size \n",
    "    newproduct,newcount,basketsize=line.split(\"\\t\")\n",
    "    newcount = int(newcount)\n",
    "    \n",
    "    # Get the total products and the set of unique products.\n",
    "    # These fields do not have to come first as we compute\n",
    "    # the relative frequencies after all stdin has been\n",
    "    # processed\n",
    "    if newproduct==\"Totals\": \n",
    "        total+=newcount\n",
    "        uniqueproducts.update(eval(basketsize))\n",
    "        continue\n",
    "    basketsize=int(basketsize)\n",
    "\n",
    "    #Update maximum basket size\n",
    "    if basketsize>maxbasketsize: \n",
    "        maxbasketsize=basketsize\n",
    "        \n",
    "    if product==newproduct:\n",
    "        count+=newcount\n",
    "    else:\n",
    "        # We are finished with the current product\n",
    "        # Update the products dictionary with the product count\n",
    "        products[product] = products.setdefault(product, 0) + count\n",
    "        product=newproduct\n",
    "        count=newcount\n",
    "        \n",
    "# Change products dictionary in an array of tuples with counts\n",
    "# and relative frequencies\n",
    "products =[(k, v,float(v)/total) for k, v in products.iteritems()]\n",
    "\n",
    "# Sort the dictionary by count and then by product name\n",
    "products   = sorted(products,key=lambda x: x[0])\n",
    "products   = sorted(products,key=lambda x: x[1])\n",
    "\n",
    "# Output the number of unique products, largest basket size, and top 50\n",
    "# products.\n",
    "print \"Number of Unique Products:\",len(uniqueproducts)\n",
    "print \"Largest Basket Size:\", maxbasketsize\n",
    "print \"Top 50 Products:\"\n",
    "for i,p in enumerate(products[-50:]):\n",
    "    print str(50-i)+\":\",p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mapreduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:56:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `results/3.3': No such file or directory\n",
      "16/06/05 11:56:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 11:56:29 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 11:56:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /tmp/hadoop-unjar1357311333018480171/] [] /tmp/streamjob72353093689334652.jar tmpDir=null\n",
      "16/06/05 11:56:29 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:56:30 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 11:56:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 11:56:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 11:56:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0192\n",
      "16/06/05 11:56:31 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0192\n",
      "16/06/05 11:56:31 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0192/\n",
      "16/06/05 11:56:31 INFO mapreduce.Job: Running job: job_1464324416493_0192\n",
      "16/06/05 11:56:39 INFO mapreduce.Job: Job job_1464324416493_0192 running in uber mode : false\n",
      "16/06/05 11:56:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 11:56:48 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/05 11:56:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 11:56:56 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 11:56:57 INFO mapreduce.Job: Job job_1464324416493_0192 completed successfully\n",
      "16/06/05 11:56:57 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6230138\n",
      "\t\tFILE: Number of bytes written=12822861\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462815\n",
      "\t\tHDFS: Number of bytes written=2380\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=60796\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=45096\n",
      "\t\tTotal time spent by all map tasks (ms)=15199\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5637\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15199\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5637\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=62255104\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=46178304\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=5468474\n",
      "\t\tMap output materialized bytes=6230144\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=6230144\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=53\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=316\n",
      "\t\tCPU time spent (ms)=5940\n",
      "\t\tPhysical memory (bytes) snapshot=699662336\n",
      "\t\tVirtual memory (bytes) snapshot=18995019776\n",
      "\t\tTotal committed heap usage (bytes)=619184128\n",
      "\t3.3\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2380\n",
      "16/06/05 11:56:57 INFO streaming.StreamJob: Output directory: results/3.3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/3.3\n",
    "\n",
    "!hdfs dfs -put -p -f ProductPurchaseData.txt\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-file mapper.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output results/3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 11:56:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Number of Unique Products: 12592\t\n",
      "Largest Basket Size: 37\t\n",
      "Top 50 Products:\t\n",
      "50: ('GRO85051', 1214, 0.0031878242967880175)\t\n",
      "49: ('DAI22896', 1219, 0.0032009537214041134)\t\n",
      "48: ('GRO81087', 1220, 0.0032035796063273323)\t\n",
      "47: ('DAI31081', 1261, 0.0033112408881793166)\t\n",
      "46: ('GRO15017', 1275, 0.003348003277104384)\t\n",
      "45: ('ELE91337', 1289, 0.0033847656660294517)\t\n",
      "44: ('DAI43223', 1290, 0.003387391550952671)\t\n",
      "43: ('SNA96271', 1295, 0.0034005209755687666)\t\n",
      "42: ('ELE59935', 1311, 0.003442535134340273)\t\n",
      "41: ('DAI88807', 1316, 0.0034556645589563684)\t\n",
      "40: ('ELE74482', 1316, 0.0034556645589563684)\t\n",
      "39: ('GRO61133', 1321, 0.003468793983572464)\t\n",
      "38: ('ELE56788', 1345, 0.003531815221729723)\t\n",
      "37: ('GRO38814', 1352, 0.0035501964161922567)\t\n",
      "36: ('SNA90094', 1390, 0.0036499800432745837)\t\n",
      "35: ('SNA93860', 1407, 0.0036946200869693085)\t\n",
      "34: ('FRO53271', 1420, 0.003728756590971157)\t\n",
      "33: ('FRO35904', 1436, 0.0037707707497426635)\t\n",
      "32: ('ELE34057', 1489, 0.003909942650673277)\t\n",
      "31: ('GRO94758', 1489, 0.003909942650673277)\t\n",
      "30: ('ELE99737', 1516, 0.003980841543600193)\t\n",
      "29: ('FRO78087', 1531, 0.00402022981744848)\t\n",
      "28: ('DAI22177', 1627, 0.004272314770077516)\t\n",
      "27: ('SNA55762', 1646, 0.00432220658361868)\t\n",
      "26: ('ELE66810', 1697, 0.0044561267147028545)\t\n",
      "25: ('FRO32293', 1702, 0.004469256139318951)\t\n",
      "24: ('DAI83733', 1712, 0.004495514988551142)\t\n",
      "23: ('ELE66600', 1713, 0.004498140873474361)\t\n",
      "22: ('GRO46854', 1756, 0.004611053925172783)\t\n",
      "21: ('DAI63921', 1773, 0.004655693968867509)\t\n",
      "20: ('GRO56726', 1784, 0.0046845787030229185)\t\n",
      "19: ('ELE74009', 1816, 0.004768607020565931)\t\n",
      "18: ('GRO30386', 1840, 0.00483162825872319)\t\n",
      "17: ('FRO85978', 1918, 0.005036447282734282)\t\n",
      "16: ('GRO71621', 1920, 0.0050416990525807195)\t\n",
      "15: ('GRO59710', 2004, 0.005262273386131126)\t\n",
      "14: ('SNA99873', 2083, 0.005469718295065437)\t\n",
      "13: ('GRO21487', 2115, 0.005553746612608449)\t\n",
      "12: ('FRO80039', 2233, 0.005863601033548306)\t\n",
      "11: ('ELE26917', 2292, 0.006018528244018234)\t\n",
      "10: ('DAI85309', 2293, 0.006021154128941453)\t\n",
      "9: ('FRO31317', 2330, 0.006118311871100561)\t\n",
      "8: ('SNA45677', 2455, 0.006446547486502951)\t\n",
      "7: ('DAI75645', 2736, 0.007184421149927526)\t\n",
      "6: ('ELE32164', 2851, 0.007486397916097725)\t\n",
      "5: ('SNA80324', 3044, 0.007993193706279015)\t\n",
      "4: ('GRO73461', 3602, 0.009458437493435288)\t\n",
      "3: ('ELE17451', 3875, 0.010175304077474108)\t\n",
      "2: ('FRO40251', 3881, 0.010191059387013424)\t\n",
      "1: ('DAI62779', 6667, 0.017506774783101905)\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.3/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4.  Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.4 - Mapper Function Code\n",
    "from __future__ import print_function\n",
    "from sets import Set\n",
    "\n",
    "import sys,re\n",
    "sys.stderr.write(\"reporter:counter:3.4,MapperCount,1\\n\")\n",
    "\n",
    "baskets =0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    # Using a set to avoid double counting in instances where a customer \n",
    "    # bought more than 1 of a product\n",
    "    basket = list(Set(re.findall(\"[\\w']+\",line)))\n",
    "    baskets +=1\n",
    "    for i,p1 in enumerate(basket):\n",
    "        # Only iterate through products we haven't seen yet to avoid \n",
    "        # duplicates\n",
    "        for p2 in basket[i+1:]:\n",
    "            # Create pairs out of the two products.  For consitency,\n",
    "            # put the alphbetically higher element first in the pair.\n",
    "            # Then emit both products and the count\n",
    "            if p2 > p1:\n",
    "                print (p1,p2,1,sep='\\t')\n",
    "            else:\n",
    "                print (p2,p1,1,sep='\\t')\n",
    "\n",
    "# Output total number of baskets with a leading space in the key for \n",
    "# order-inversion purposes\n",
    "print (\" Total\",\"Baskets\",baskets,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "# combiner.py\n",
    "# Author:Jackson Lane\n",
    "# Description: combiner code for HW3.4\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:3.4,Combiner,1\\n\")\n",
    "product1 =''\n",
    "product2 = ''\n",
    "count =0 \n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Parse the two product fields and the count field from the mapper\n",
    "    newproduct1,newproduct2,newcount=line.split(\"\\t\")\n",
    "    newcount = int(newcount)\n",
    "    \n",
    "    # Pass along the total number of baskets to the reducer\n",
    "    if newproduct1==\"Total\":\n",
    "        print (\" Total\",\"Baskets\",newcount,sep='\\t')\n",
    "        continue\n",
    "    if product1 == newproduct1 and product2 == newproduct2:\n",
    "        count+=newcount\n",
    "    else:\n",
    "        if(count > 0): print (product1,product2,count,sep='\\t')\n",
    "        product1=newproduct1\n",
    "        product2=newproduct2\n",
    "        count=newcount\n",
    "\n",
    "if(count > 0): print (product1,product2,count,sep='\\t')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# reducer.py\n",
    "# Author:Jackson Lane\n",
    "# Description: reducer code for HW3.2.4\n",
    "# Since we are using a single reducer, it has to sum up the word counts and get the top 50 and bottom 10.\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:3.4,ReducerCount,1\\n\")\n",
    "total = float(0)\n",
    "baskets = float(0)\n",
    "pair =('','')\n",
    "count =0 \n",
    "pairs = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    # Parse the two product fields and the count field from the mapper\n",
    "    newproduct1,newproduct2,newcount=line.split(\"\\t\")\n",
    "    newcount = int(newcount)\n",
    "    #Extract the total number of baskets\n",
    "    #This field should come first in the stdin due to order inversion\n",
    "    if newproduct1==\"Total\": \n",
    "        baskets+=newcount\n",
    "        continue\n",
    "    # Turn the two products into a tuple.  This is more for convience\n",
    "    # than functionality, as it's easier to compare two tuples than \n",
    "    # four values.  Note that we will end up outputting the two products\n",
    "    # separately\n",
    "    newpair = (newproduct1,newproduct2)\n",
    "    if (newpair == pair):\n",
    "        count+=newcount\n",
    "    else:\n",
    "        #Only consider products that were purchased more than 100 times\n",
    "        if (count > 100): \n",
    "            print (pair[0],pair[1],count,count / baskets, sep = \",\")\n",
    "        pair=newpair\n",
    "        count=newcount\n",
    "#Emit the last product and count (if its over 100 of courses)\n",
    "if (count > 100):  print (pair[0],pair[1],count,count / baskets, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mapreduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 13:19:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:19:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted temp\n",
      "16/06/05 13:19:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:19:43 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 13:19:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /tmp/hadoop-unjar2150924994250287621/] [] /tmp/streamjob7177716152771347565.jar tmpDir=null\n",
      "16/06/05 13:19:44 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:19:44 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:19:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 13:19:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 13:19:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0230\n",
      "16/06/05 13:19:45 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0230\n",
      "16/06/05 13:19:45 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0230/\n",
      "16/06/05 13:19:45 INFO mapreduce.Job: Running job: job_1464324416493_0230\n",
      "16/06/05 13:19:52 INFO mapreduce.Job: Job job_1464324416493_0230 running in uber mode : false\n",
      "16/06/05 13:19:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 13:20:02 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/06/05 13:20:03 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/05 13:20:12 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/06/05 13:20:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 13:20:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 13:20:23 INFO mapreduce.Job: Job job_1464324416493_0230 completed successfully\n",
      "16/06/05 13:20:23 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22624203\n",
      "\t\tFILE: Number of bytes written=45614783\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462815\n",
      "\t\tHDFS: Number of bytes written=52179\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=145148\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=69496\n",
      "\t\tTotal time spent by all map tasks (ms)=36287\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8687\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=36287\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8687\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=148631552\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=71163904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534016\n",
      "\t\tMap output bytes=50680322\n",
      "\t\tMap output materialized bytes=22624209\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=2534016\n",
      "\t\tCombine output records=1026709\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=22624209\n",
      "\t\tReduce input records=1026709\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=2053418\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=636\n",
      "\t\tCPU time spent (ms)=32120\n",
      "\t\tPhysical memory (bytes) snapshot=1332830208\n",
      "\t\tVirtual memory (bytes) snapshot=18996748288\n",
      "\t\tTotal committed heap usage (bytes)=1266679808\n",
      "\t3.4\n",
      "\t\tCombiner=2\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52179\n",
      "16/06/05 13:20:23 INFO streaming.StreamJob: Output directory: temp\n",
      "16/06/05 13:20:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:20:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/3.4\n",
      "16/06/05 13:20:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:20:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar2933380005635235536/] [] /tmp/streamjob2570496342596996042.jar tmpDir=null\n",
      "16/06/05 13:20:29 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:20:30 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:20:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 13:20:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 13:20:31 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/06/05 13:20:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0231\n",
      "16/06/05 13:20:31 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0231\n",
      "16/06/05 13:20:31 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0231/\n",
      "16/06/05 13:20:31 INFO mapreduce.Job: Running job: job_1464324416493_0231\n",
      "16/06/05 13:20:38 INFO mapreduce.Job: Job job_1464324416493_0231 running in uber mode : false\n",
      "16/06/05 13:20:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 13:20:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 13:20:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 13:20:50 INFO mapreduce.Job: Job job_1464324416493_0231 completed successfully\n",
      "16/06/05 13:20:50 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=56118\n",
      "\t\tFILE: Number of bytes written=475145\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=56461\n",
      "\t\tHDFS: Number of bytes written=53490\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23944\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22920\n",
      "\t\tTotal time spent by all map tasks (ms)=5986\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2865\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5986\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2865\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=24518656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=23470080\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1311\n",
      "\t\tMap output records=1311\n",
      "\t\tMap output bytes=53490\n",
      "\t\tMap output materialized bytes=56124\n",
      "\t\tInput split bytes=186\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1311\n",
      "\t\tReduce shuffle bytes=56124\n",
      "\t\tReduce input records=1311\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=2622\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=206\n",
      "\t\tCPU time spent (ms)=2160\n",
      "\t\tPhysical memory (bytes) snapshot=691863552\n",
      "\t\tVirtual memory (bytes) snapshot=18991587328\n",
      "\t\tTotal committed heap usage (bytes)=643301376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56275\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53490\n",
      "16/06/05 13:20:50 INFO streaming.StreamJob: Output directory: results/3.4\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r temp\n",
    "\n",
    "!hdfs dfs -put -p -f ProductPurchaseData.txt\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k1,2' \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D stream.num.reduce.output.key.fields=2 \\\n",
    "-file mapper.py \\\n",
    "-file combiner.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-combiner \"combiner.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output temp\n",
    "\n",
    "!hdfs dfs -rm -r results/3.4\n",
    "\n",
    "# Second map reduce job to sort the output.\n",
    "!hdfs dfs -put -p -f ProductPurchaseData.txt\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k3,3nr -k1,2' \\\n",
    "-D mapreduce.output.key.field.separator=\",\" \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.reduce.output.field.separator=, \\\n",
    "-D stream.map.input.field.separator=, \\\n",
    "-D stream.reduce.input.field.separator=, \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=4 \\\n",
    "-D stream.num.reduce.output.key.fields=4 \\\n",
    "-mapper cat \\\n",
    "-reducer cat \\\n",
    "-input temp/part-* \\\n",
    "-output results/3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 13:19:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779,ELE17451,1592,0.0511880646925\t\t\n",
      "FRO40251,SNA80324,1412,0.0454004694383\t\t\n",
      "DAI75645,FRO40251,1254,0.0403202469374\t\t\n",
      "FRO40251,GRO85051,1213,0.0390019613517\t\t\n",
      "DAI62779,GRO73461,1139,0.0366226166361\t\t\n",
      "DAI75645,SNA80324,1130,0.0363332368734\t\t\n",
      "DAI62779,FRO40251,1070,0.0344040384554\t\t\n",
      "DAI62779,SNA80324,923,0.0296775023311\t\t\n",
      "DAI62779,DAI85309,918,0.0295167357963\t\t\n",
      "ELE32164,GRO59710,911,0.0292916626475\t\t\n",
      "DAI62779,DAI75645,882,0.0283592167454\t\t\n",
      "FRO40251,GRO73461,882,0.0283592167454\t\t\n",
      "DAI62779,ELE92920,877,0.0281984502106\t\t\n",
      "FRO40251,FRO92469,835,0.026848011318\t\t\n",
      "DAI62779,ELE32164,832,0.0267515513971\t\t\n",
      "DAI75645,GRO73461,712,0.0228931545609\t\t\n",
      "DAI43223,ELE32164,711,0.022861001254\t\t\n",
      "DAI62779,GRO30386,709,0.02279669464\t\t\n",
      "ELE17451,FRO40251,697,0.0224108549564\t\t\n",
      "DAI85309,ELE99737,659,0.0211890292917\t\t\n",
      "DAI62779,ELE26917,650,0.020899649529\t\t\n",
      "GRO21487,GRO73461,631,0.0202887366966\t\t\n",
      "DAI62779,SNA45677,604,0.0194205974084\t\t\n",
      "ELE17451,SNA80324,597,0.0191955242597\t\t\n",
      "DAI62779,GRO71621,595,0.0191312176457\t\t\n",
      "DAI62779,SNA55762,593,0.0190669110318\t\t\n",
      "DAI62779,DAI83733,586,0.018841837883\t\t\n",
      "ELE17451,GRO73461,580,0.0186489180412\t\t\n",
      "GRO73461,SNA80324,562,0.0180701585158\t\t\n",
      "DAI62779,GRO59710,561,0.0180380052088\t\t\n",
      "DAI62779,FRO80039,550,0.0176843188322\t\t\n",
      "DAI75645,ELE17451,547,0.0175878589113\t\t\n",
      "DAI62779,SNA93860,537,0.0172663258416\t\t\n",
      "DAI55148,DAI62779,526,0.016912639465\t\t\n",
      "DAI43223,GRO59710,512,0.0164624931674\t\t\n",
      "ELE17451,ELE32164,511,0.0164303398605\t\t\n",
      "DAI62779,SNA18336,506,0.0162695733256\t\t\n",
      "ELE32164,GRO73461,486,0.0156265071863\t\t\n",
      "DAI62779,FRO78087,482,0.0154978939584\t\t\n",
      "DAI85309,ELE17451,482,0.0154978939584\t\t\n",
      "DAI62779,GRO94758,479,0.0154014340375\t\t\n",
      "DAI62779,GRO21487,471,0.0151442075817\t\t\n",
      "GRO85051,SNA80324,471,0.0151442075817\t\t\n",
      "ELE17451,GRO30386,468,0.0150477476608\t\t\n",
      "FRO85978,SNA95666,463,0.014886981126\t\t\n",
      "DAI62779,FRO19221,462,0.014854827819\t\t\n",
      "DAI62779,GRO46854,461,0.0148226745121\t\t\n",
      "DAI43223,DAI62779,459,0.0147583678981\t\t\n",
      "ELE92920,SNA18336,455,0.0146297546703\t\t\n",
      "DAI88079,FRO40251,446,0.0143403749076\t\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.4/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran both jobs on a Softlayer Hadoop 2.7.2 cluster with 1 master and 2 slave VMs.  Each VM is running CentOS7.0-64 on 2 2.0 GHz cores with 6GB of RAM.  \n",
    "\n",
    "In each map reduce jobs, there were two mapper and one reducer tasks.  The combiner was called twice.  \n",
    "\n",
    "According to the output, the entire MapReduce job ran in 44 seconds.  The ,apper part ran in 36 seconds while the reducer part ran in 8 seconds.  The second map reduce job ran in 34 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW 3.4 - Mapper Function Code\n",
    "from __future__ import print_function\n",
    "from sets import Set\n",
    "\n",
    "import sys,re\n",
    "sys.stderr.write(\"reporter:counter:3.5,MapperCount,1\\n\")\n",
    "\n",
    "#Define data split for custom partitioner\n",
    "baskets =0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line=line.strip()\n",
    "    # Using a set to avoid double counting in instances where a customer bought more than 1 of a product\n",
    "    basket = sorted(Set(re.findall(\"[\\w']+\",line)))\n",
    "    baskets +=1\n",
    "    for i,p1 in enumerate(basket[:-1]):\n",
    "        stripe = dict([(x,1) for x in basket[i+1:]])\n",
    "        print (p1,stripe,sep='\\t')\n",
    "        \n",
    "#Output total number of carts with a special key for order-inversion purposes\n",
    "print (\" Total\",baskets,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "# combiner.py\n",
    "# Author:Jackson Lane\n",
    "# Description: combiner code for HW3.5\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:3.5,Combiner,1\\n\")\n",
    "product =''\n",
    "stripe = {}\n",
    "for line in sys.stdin:\n",
    "    #Parse line into fields\n",
    "    sys.stderr.write(line)\n",
    "\n",
    "    line = line.strip()\n",
    "    newproduct,newstripe=line.split(\"\\t\")\n",
    "    if newproduct==\"Total\": #Extract total products for order inversion\n",
    "        print (\" Total\",int(newstripe)  ,sep='\\t')\n",
    "        continue\n",
    "    newstripe = eval(newstripe)\n",
    "    \n",
    "    if product == newproduct:\n",
    "        stripe = { k: stripe.get(k, 0) + newstripe.get(k, 0) for k in set(stripe) | set(newstripe) }\n",
    "    else:\n",
    "        if len(stripe) > 0: print (product,stripe,sep='\\t')\n",
    "        product=newproduct\n",
    "        stripe=newstripe\n",
    "\n",
    "if len(stripe) > 0: print (product,stripe,sep='\\t')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# reducer.py\n",
    "# Author:Jackson Lane\n",
    "# Description: reducer code for HW3.5\n",
    "# Reads in stripes pattern from mapper and breaks up into product pairs\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:3.5,ReducerCount,1\\n\")\n",
    "baskets = float(0)\n",
    "product =''\n",
    "stripe = {}\n",
    "for line in sys.stdin:\n",
    "    #Parse line into fields\n",
    "    sys.stderr.write(line)\n",
    "    line = line.strip()\n",
    "    newproduct,newstripe=line.split(\"\\t\")\n",
    "    if newproduct==\"Total\": #Extract total products for order inversion\n",
    "        baskets+=int(newstripe)        \n",
    "        continue\n",
    "    newstripe = eval(newstripe)\n",
    "    \n",
    "    if product == newproduct:\n",
    "        stripe = { k: stripe.get(k, 0) + newstripe.get(k, 0) for k in set(stripe) | set(newstripe) }\n",
    "    else:\n",
    "        for (product2,count) in stripe.iteritems():\n",
    "            count = int(count)\n",
    "            if (count > 100): print(product,product2,count,count / baskets, sep = \",\")          \n",
    "        product=newproduct\n",
    "        stripe=newstripe\n",
    "\n",
    "for (product2,count) in stripe.iteritems():\n",
    "            count = int(count)\n",
    "            pair = (product,product2)\n",
    "            if (count > 100): print(product,product2,count,count / baskets, sep= \",\")          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mapreduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 13:43:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:43:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted temp\n",
      "16/06/05 13:43:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:43:43 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/06/05 13:43:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /tmp/hadoop-unjar303191613663761740/] [] /tmp/streamjob4794543400269878294.jar tmpDir=null\n",
      "16/06/05 13:43:44 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:43:45 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:43:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 13:43:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 13:43:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0242\n",
      "16/06/05 13:43:46 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0242\n",
      "16/06/05 13:43:46 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0242/\n",
      "16/06/05 13:43:46 INFO mapreduce.Job: Running job: job_1464324416493_0242\n",
      "16/06/05 13:43:53 INFO mapreduce.Job: Job job_1464324416493_0242 running in uber mode : false\n",
      "16/06/05 13:43:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 13:44:05 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/05 13:45:08 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/06/05 13:45:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 13:45:19 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/06/05 13:45:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 13:45:21 INFO mapreduce.Job: Job job_1464324416493_0242 completed successfully\n",
      "16/06/05 13:45:21 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15696092\n",
      "\t\tFILE: Number of bytes written=31758558\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462815\n",
      "\t\tHDFS: Number of bytes written=52179\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=590408\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=70120\n",
      "\t\tTotal time spent by all map tasks (ms)=147602\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8765\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=147602\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8765\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=604577792\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=71802880\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=349722\n",
      "\t\tMap output bytes=42019242\n",
      "\t\tMap output materialized bytes=15696098\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=349722\n",
      "\t\tCombine output records=16942\n",
      "\t\tReduce input groups=16930\n",
      "\t\tReduce shuffle bytes=15696098\n",
      "\t\tReduce input records=16942\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=33884\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=459\n",
      "\t\tCPU time spent (ms)=138410\n",
      "\t\tPhysical memory (bytes) snapshot=840298496\n",
      "\t\tVirtual memory (bytes) snapshot=19004043264\n",
      "\t\tTotal committed heap usage (bytes)=694681600\n",
      "\t3.5\n",
      "\t\tCombiner=2\n",
      "\t\tMapperCount=2\n",
      "\t\tReducerCount=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52179\n",
      "16/06/05 13:45:21 INFO streaming.StreamJob: Output directory: temp\n",
      "16/06/05 13:45:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:45:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/3.5\n",
      "16/06/05 13:45:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/05 13:45:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar2395628589614226864/] [] /tmp/streamjob8858589019399778405.jar tmpDir=null\n",
      "16/06/05 13:45:27 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:45:27 INFO client.RMProxy: Connecting to ResourceManager at /50.23.93.133:8032\n",
      "16/06/05 13:45:28 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 13:45:28 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 13:45:28 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/06/05 13:45:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464324416493_0243\n",
      "16/06/05 13:45:28 INFO impl.YarnClientImpl: Submitted application application_1464324416493_0243\n",
      "16/06/05 13:45:28 INFO mapreduce.Job: The url to track the job: http://50.23.93.133:8088/proxy/application_1464324416493_0243/\n",
      "16/06/05 13:45:28 INFO mapreduce.Job: Running job: job_1464324416493_0243\n",
      "16/06/05 13:45:36 INFO mapreduce.Job: Job job_1464324416493_0243 running in uber mode : false\n",
      "16/06/05 13:45:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 13:45:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 13:45:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 13:45:49 INFO mapreduce.Job: Job job_1464324416493_0243 completed successfully\n",
      "16/06/05 13:45:49 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=54807\n",
      "\t\tFILE: Number of bytes written=472523\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=56461\n",
      "\t\tHDFS: Number of bytes written=52179\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=37796\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=26816\n",
      "\t\tTotal time spent by all map tasks (ms)=9449\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3352\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9449\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3352\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=38703104\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27459584\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1311\n",
      "\t\tMap output records=1311\n",
      "\t\tMap output bytes=52179\n",
      "\t\tMap output materialized bytes=54813\n",
      "\t\tInput split bytes=186\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1311\n",
      "\t\tReduce shuffle bytes=54813\n",
      "\t\tReduce input records=1311\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=2622\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=285\n",
      "\t\tCPU time spent (ms)=2250\n",
      "\t\tPhysical memory (bytes) snapshot=692301824\n",
      "\t\tVirtual memory (bytes) snapshot=18996404224\n",
      "\t\tTotal committed heap usage (bytes)=623378432\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56275\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52179\n",
      "16/06/05 13:45:49 INFO streaming.StreamJob: Output directory: results/3.5\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r temp\n",
    "\n",
    "!hdfs dfs -put -p -f ProductPurchaseData.txt\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k1,1' \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D stream.num.reduce.output.key.fields=2 \\\n",
    "-file mapper.py \\\n",
    "-file combiner.py \\\n",
    "-file reducer.py \\\n",
    "-mapper \"mapper.py\" \\\n",
    "-combiner \"combiner.py\" \\\n",
    "-reducer \"reducer.py\" \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output temp\n",
    "\n",
    "!hdfs dfs -rm -r results/3.5\n",
    "\n",
    "# Second map reduce job to sort the output.\n",
    "!hdfs dfs -put -p -f ProductPurchaseData.txt\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2*.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options='-k3,3nr -k1,2' \\\n",
    "-D mapreduce.output.key.field.separator=\",\" \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.reduce.output.field.separator=, \\\n",
    "-D stream.map.input.field.separator=, \\\n",
    "-D stream.reduce.input.field.separator=, \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.num.reduce.output.key.fields=3 \\\n",
    "-mapper cat \\\n",
    "-reducer cat \\\n",
    "-input temp/part-* \\\n",
    "-output results/3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 13:45:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779,ELE17451,1592,0.0511880646925\t\t\n",
      "FRO40251,SNA80324,1412,0.0454004694383\t\t\n",
      "DAI75645,FRO40251,1254,0.0403202469374\t\t\n",
      "FRO40251,GRO85051,1213,0.0390019613517\t\t\n",
      "DAI62779,GRO73461,1139,0.0366226166361\t\t\n",
      "DAI75645,SNA80324,1130,0.0363332368734\t\t\n",
      "DAI62779,FRO40251,1070,0.0344040384554\t\t\n",
      "DAI62779,SNA80324,923,0.0296775023311\t\t\n",
      "DAI62779,DAI85309,918,0.0295167357963\t\t\n",
      "ELE32164,GRO59710,911,0.0292916626475\t\t\n",
      "DAI62779,DAI75645,882,0.0283592167454\t\t\n",
      "FRO40251,GRO73461,882,0.0283592167454\t\t\n",
      "DAI62779,ELE92920,877,0.0281984502106\t\t\n",
      "FRO40251,FRO92469,835,0.026848011318\t\t\n",
      "DAI62779,ELE32164,832,0.0267515513971\t\t\n",
      "DAI75645,GRO73461,712,0.0228931545609\t\t\n",
      "DAI43223,ELE32164,711,0.022861001254\t\t\n",
      "DAI62779,GRO30386,709,0.02279669464\t\t\n",
      "ELE17451,FRO40251,697,0.0224108549564\t\t\n",
      "DAI85309,ELE99737,659,0.0211890292917\t\t\n",
      "DAI62779,ELE26917,650,0.020899649529\t\t\n",
      "GRO21487,GRO73461,631,0.0202887366966\t\t\n",
      "DAI62779,SNA45677,604,0.0194205974084\t\t\n",
      "ELE17451,SNA80324,597,0.0191955242597\t\t\n",
      "DAI62779,GRO71621,595,0.0191312176457\t\t\n",
      "DAI62779,SNA55762,593,0.0190669110318\t\t\n",
      "DAI62779,DAI83733,586,0.018841837883\t\t\n",
      "ELE17451,GRO73461,580,0.0186489180412\t\t\n",
      "GRO73461,SNA80324,562,0.0180701585158\t\t\n",
      "DAI62779,GRO59710,561,0.0180380052088\t\t\n",
      "DAI62779,FRO80039,550,0.0176843188322\t\t\n",
      "DAI75645,ELE17451,547,0.0175878589113\t\t\n",
      "DAI62779,SNA93860,537,0.0172663258416\t\t\n",
      "DAI55148,DAI62779,526,0.016912639465\t\t\n",
      "DAI43223,GRO59710,512,0.0164624931674\t\t\n",
      "ELE17451,ELE32164,511,0.0164303398605\t\t\n",
      "DAI62779,SNA18336,506,0.0162695733256\t\t\n",
      "ELE32164,GRO73461,486,0.0156265071863\t\t\n",
      "DAI62779,FRO78087,482,0.0154978939584\t\t\n",
      "DAI85309,ELE17451,482,0.0154978939584\t\t\n",
      "DAI62779,GRO94758,479,0.0154014340375\t\t\n",
      "DAI62779,GRO21487,471,0.0151442075817\t\t\n",
      "GRO85051,SNA80324,471,0.0151442075817\t\t\n",
      "ELE17451,GRO30386,468,0.0150477476608\t\t\n",
      "FRO85978,SNA95666,463,0.014886981126\t\t\n",
      "DAI62779,FRO19221,462,0.014854827819\t\t\n",
      "DAI62779,GRO46854,461,0.0148226745121\t\t\n",
      "DAI43223,DAI62779,459,0.0147583678981\t\t\n",
      "ELE92920,SNA18336,455,0.0146297546703\t\t\n",
      "DAI88079,FRO40251,446,0.0143403749076\t\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat results/3.4/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran both jobs on a Softlayer Hadoop 2.7.2 cluster with 1 master and 2 slave VMs.  Each VM is running CentOS7.0-64 on 2 2.0 GHz cores with 6GB of RAM.  \n",
    "\n",
    "In each map reduce jobs, there were two mapper and one reducer tasks, and the counters reflected this.  The combiner was called twice.  \n",
    "\n",
    "According to the output, the entire MapReduce job ran in 101 seconds.  The Mapper part ran in 93 seconds while the reducer part ran in 8 seconds.  The second map reduce job ran in 26 seconds.\n",
    "\n",
    "The stripes job definetely ran slower than pairs job.  I believe this is because the Mapper processed the stripe as a key, and that processing slowed the mapper down signficantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
