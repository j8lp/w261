{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 1\n",
    "Jackson Lane (jelane@berkeley.edu) <br>\n",
    "W261-3 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW1.0.0: Define big data. Provide an example of a big data problem in your domain of expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data means data that is too big or complex for traditional data analysis tools.  Its distinguishing characteristics include the four Vs.  Big data exists in the context of a Big data problem, which may involve performing a certain type of analysis on one or more very large and/or multidimensional datasets in a limited period of time.  For example, 50 pages of text alone is probably not big data, but if a human has to read 50 pages in just 5 minutes, then that text becomes Big data in the context of the Big data problem of parsing a large corpus of text in semi-realtime: To achieve the desired speed of analysis, you will need to special tools like NLP techniques or parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expected total error in any model will always be:\n",
    "\n",
    "$$\n",
    "E[\\hat{y}-y]^2 = Bias^2 + IrreducibleError^2 + Variance$$\n",
    "\n",
    "To estimate variance for each degree, I'd run at least 100 different models of that degree across random subsamples of the test dataset T.  Then for each degree, variance is:\n",
    "\n",
    "$$\n",
    "E[\\hat{y} - h(\\hat{y})]^2\n",
    "$$\n",
    "\n",
    "Where h(y) is the mean value of the models' predictions.  In other words, variance is simply the standard deviation squared of the predictions of the 100 models.  It can be calculated without knowing the true function or value.\n",
    "\n",
    "Bias is estimated as:\n",
    "\n",
    "$$\n",
    "f(y) - h(\\hat{y})\n",
    "$$\n",
    "\n",
    "and irreducible error is estimated as:\n",
    "\n",
    "$$\n",
    "E[y - f(y)]\n",
    "$$\n",
    "\n",
    "where f(y) reperesents the value of the true function f and y represents the true value as observed in reality.  However, it's difficult to estiamte bias and irreducible error because it requires knowledge of the true function ahead of time.  While one can observe the true values from experiments and calculate variance from samples, it's mathematically impossible to derive the true function from just data alone.  \n",
    "\n",
    "But one can still estimate the sum of irreducible error squared and bias squared by subtracting the variance from the model squared error.  \n",
    "\n",
    "$$\n",
    "IrreducibleError^2 + Bias^2= E[\\hat{y}-y]^2 - Variance\n",
    "$$\n",
    "\n",
    "Furthermore, one can minimize bias by using a high degree polynomial (such as 12).  So if one subtracts the variance from the model squared error for a model with polynomial 12, one can get a pretty good estimate of the irreducible error.  \n",
    "\n",
    "$$\n",
    "IrreducibleError^2 + Bias_{->0}^2= IrreducibleError^2 = E[\\hat{y}-y]^2 - Variance \n",
    "$$\n",
    "\n",
    "By defintion, irreducible error should remain constant regardless of which degree polynomial we use.  Since we now have an estimate for irreducible error, we can estimate bias for all the other degress by subtracting variance and irreducible error squared from the model squared error.\n",
    "\n",
    "$$\n",
    "Bias^2= E[\\hat{y}-y]^2 - Variance - IrreducibleError^2\n",
    "$$\n",
    "\n",
    "But in the end however, I'd still choose my model based off just mean squared error alone.  Whether that error can be attributed to bias or variance is not really as relevant to me.  However, I'd make sure to keep a set of hold out data that I will only use to test my model against after my model is training.  If use every record in the data fr both training and testing, then I'll end up with a model that probably doesn't generalize well to the rest of the population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author: Jackson Lane\n",
    "# Description: mapper code for HW1.2-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "# collect user input\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2].lower()\n",
    "        \n",
    "file = open (filename, \"r\")\n",
    "for line in file.readlines() :\n",
    "    line = re.sub('\\n','',line)\n",
    "    #Parse each line and get the textual part of the e-mail\n",
    "    [email,spam,subject,body] = re.split(\"\\t\",line)            \n",
    "    data = body.lower() + subject.lower()\n",
    "    #Split each email into a list of words\n",
    "    words = re.split('\\W+',data)\n",
    "    for word in words:\n",
    "        #Emit if a word matches the findword\n",
    "        if (word == findword): print word , 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "# reducer.py\n",
    "# Author: Jackson Lane\n",
    "# Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "countfiles = sys.argv[1:len(sys.argv)]\n",
    "word = \"\"\n",
    "total = 0\n",
    "\n",
    "# loop over the files produced by the mapper\n",
    "for filename in countfiles:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            #Get rid of the newline character strangly \n",
    "            line = re.sub('\\n','',line)\n",
    "            [word,count] = line.split()\n",
    "            #'count' is the number of times that the mapper said this word appeared\n",
    "            #It will always be 1 for this assignment, \n",
    "            #but might vary once we start using combiners in week 3\n",
    "            count = int(count)\n",
    "            #Add 1 to total number of instances of a word\n",
    "            total += count\n",
    "                    \n",
    "print \"The word \\\"\" , word, \"\\\" appeared \" ,total ,\" times in the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.2:\n",
    "Note that the word assistance appears only 9 times total in the email's contents,\n",
    "and 1 time in the email's subjects. This implementation and those below focus strictly on the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \" assistance \" appeared  10  times in the dataset.\r\n"
     ]
    }
   ],
   "source": [
    "# change permissions and run the naive bayes shell script\n",
    "!chmod +x mapper.py; chmod +x reducer.py\n",
    "!chmod +x pNaiveBayes.sh;\n",
    "!./pNaiveBayes.sh 4 \"assistance\"; cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "# mapper.py\n",
    "# Author: Jackson Lane\n",
    "# Description: mapper code for HW1.3-1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "filename = sys.argv[1]\n",
    "# get list of user specified words\n",
    "findwords = []\n",
    "if(len(sys.argv) > 2):\n",
    "    findwords = re.split(\" \",sys.argv[2].lower())\n",
    "        \n",
    "file = open (filename, \"r\")\n",
    "for line in file.readlines() :\n",
    "    line = re.sub('\\n','',line)\n",
    "    [email,spam,subject,content] = re.split(\"\\t\",line)            \n",
    "    data = content.lower() + subject.lower()\n",
    "    words = re.split('\\W+',data)\n",
    "    for word in words:\n",
    "        #Flag if word is in findwords list\n",
    "        if word in findwords:\n",
    "            flag=1\n",
    "        else: flag =0\n",
    "        #Emit ID, label, word, and flag\n",
    "        print [email,spam,word,flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.3 - Reducer function\n",
    "#Description: Reducer code for HW 1.3 - 1.4\n",
    "\n",
    "#HW 1.3 - Reducer Function Code\n",
    "from __future__ import division #Python 3-style division syntax is much cleaner\n",
    "import sys\n",
    " \n",
    "words={}\n",
    "emails={}\n",
    "filenames = sys.argv[1:]\n",
    "spam_email_count=0 #number of spam emails\n",
    "ham_email_count=0 #number of ham emails\n",
    "spam_word_count=0 #number of words in spam emails\n",
    "ham_word_count=0 #number of words in ham emails\n",
    "for file in filenames:\n",
    "    # Train classifier with data from mapper.py\n",
    "    with open(file, \"r\") as opened:\n",
    "        for line in opened.readlines():\n",
    "\n",
    "            #parse the incoming line\n",
    "            [email,spam,word,flag]=eval(line)\n",
    "            spam=int(spam)\n",
    "            flag=int(flag)\n",
    "\n",
    "            # If a word is flagged, then record whether it appeared in spam or ham\n",
    "            if flag==1:\n",
    "                if spam==1:\n",
    "                    words.setdefault(word,{'ham_count':0,'spam_count':0})[\"spam_count\"]+=1\n",
    "                else:   \n",
    "                    words.setdefault(word,{'ham_count':0,'spam_count':0})[\"ham_count\"]+=1\n",
    "                    \n",
    "            # Count total number of words in each class\n",
    "            if spam==1:\n",
    "                  spam_word_count+=1\n",
    "            else: \n",
    "                ham_word_count+=1\n",
    "            \n",
    "            #Count total number of e-mails in each class\n",
    "            if email not in emails:\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                else: \n",
    "                    ham_email_count+=1\n",
    "                emails[email] = {'spam':spam,'word_count':0,'words':[]}\n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate priors\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=ham_email_count / len(emails)\n",
    "for k,word in words.iteritems():\n",
    "    word['p_spam']=(word['spam_count'])/(spam_word_count)\n",
    "    word['p_ham']=(word['ham_count'])/(ham_word_count)\n",
    "\n",
    "#Accuracy here refers to amount gotten right\n",
    "accuracy =0\n",
    "for j,email in emails.iteritems():\n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    for word in email['words']:\n",
    "        if word in words:\n",
    "            #Multiply priors by conditional probabilities to get posteriors.\n",
    "            p_spam*=(words[word]['p_spam'])\n",
    "            p_ham*=(words[word]['p_ham'])\n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "    #Increment accuracy count if made correct predictin\n",
    "    if (spam_pred == email['spam']): accuracy += 1\n",
    "    #Print prediction vs actual\n",
    "    print j,'\\t',email['spam'],'\\t',spam_pred\n",
    "\n",
    "print \"Accuracy: \", accuracy / len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.3 - Results\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t1\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\n",
      "0008.2004-08-01.BG \t1 \t0\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t0\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t0\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t1\n",
      "0005.2003-12-18.GP \t1 \t0\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t0\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t0\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0007.2003-12-18.GP \t1 \t0\n",
      "0017.2004-08-02.BG \t1 \t0\n",
      "0014.2004-08-01.BG \t1 \t0\n",
      "0006.2003-12-18.GP \t1 \t0\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\n",
      "0008.2003-12-18.GP \t1 \t0\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t0\n",
      "0004.2004-08-01.BG \t1 \t0\n",
      "0018.2003-12-18.GP \t1 \t1\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t0\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t0\n",
      "0006.2004-08-01.BG \t1 \t0\n",
      "0009.2003-12-18.GP \t1 \t0\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t0\n",
      "0017.2004-08-01.BG \t1 \t0\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t1\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t0\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t0\n",
      "0003.2003-12-18.GP \t1 \t0\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t0\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t0\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t1\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\n",
      "0011.2003-12-18.GP \t1 \t0\n",
      "Accuracy:  0.6\n"
     ]
    }
   ],
   "source": [
    "#Run our HW 1.3 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!echo \"HW 1.3 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.4 - Results\n",
      "0010.2003-12-18.GP \t1 \t0\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t1\n",
      "0011.2001-06-29.SA_and_HP \t1 \t0\n",
      "0008.2004-08-01.BG \t1 \t0\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t0\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\n",
      "0015.2001-07-05.SA_and_HP \t1 \t0\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t0\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t0\n",
      "0008.2001-06-12.SA_and_HP \t1 \t0\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t0\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t1\n",
      "0005.2003-12-18.GP \t1 \t0\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t0\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t0\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0007.2003-12-18.GP \t1 \t0\n",
      "0017.2004-08-02.BG \t1 \t0\n",
      "0014.2004-08-01.BG \t1 \t0\n",
      "0006.2003-12-18.GP \t1 \t0\n",
      "0016.2001-07-05.SA_and_HP \t1 \t0\n",
      "0008.2003-12-18.GP \t1 \t0\n",
      "0014.2001-07-04.SA_and_HP \t1 \t0\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t0\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t0\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t0\n",
      "0004.2004-08-01.BG \t1 \t0\n",
      "0018.2003-12-18.GP \t1 \t1\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t1\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t0\n",
      "0006.2004-08-01.BG \t1 \t0\n",
      "0009.2003-12-18.GP \t1 \t1\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t0\n",
      "0017.2004-08-01.BG \t1 \t1\n",
      "0013.2001-06-30.SA_and_HP \t1 \t0\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t1\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t0\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t0\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t0\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t0\n",
      "0003.2003-12-18.GP \t1 \t0\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t0\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t0\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t1\n",
      "0002.2001-05-25.SA_and_HP \t1 \t0\n",
      "0011.2003-12-18.GP \t1 \t0\n",
      "Accuracy:  0.63\n"
     ]
    }
   ],
   "source": [
    "#Uses same mapper and reducer as previous problem.\n",
    "#Run our HW 1.4 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\"\n",
    "!echo \"HW 1.4 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW 1.3 - Reducer function\n",
    "#Same as reducer for 1.3,1.4\n",
    "#Except that this time we don't take into account whether a word is flagged\n",
    "from __future__ import division #Python 3-style division syntax is much cleaner\n",
    "import sys, math\n",
    " \n",
    "words={}\n",
    "emails={}\n",
    "filenames = sys.argv[1:]\n",
    "spam_email_count=0 #number of spam emails\n",
    "ham_email_count=0 #number of ham emails\n",
    "spam_word_count=0 #number of words in spam emails\n",
    "ham_word_count=0 #number of words in ham emails\n",
    "for file in filenames:\n",
    "    # Train classifier with data from mapper.py\n",
    "    with open(file, \"r\") as opened:\n",
    "        for line in opened.readlines():\n",
    "\n",
    "            #parse the incoming line\n",
    "            line=eval(line)\n",
    "            email=line[0]\n",
    "            spam=int(line[1])\n",
    "            word=line[2]\n",
    "            flag=int(line[3])\n",
    "            if spam==1:\n",
    "                #using +1 smoothing\n",
    "                words.setdefault(word,{'ham_count':1,'spam_count':1})[\"spam_count\"]+=1\n",
    "                spam_word_count+=1\n",
    "            else: \n",
    "                #using +1 smoothing\n",
    "                words.setdefault(word,{'ham_count':1,'spam_count':1})[\"ham_count\"]+=1\n",
    "                ham_word_count+=1\n",
    "            #store email data \n",
    "            if(email not in emails.keys()):\n",
    "                if spam==1:\n",
    "                    spam_email_count+=1\n",
    "                else: \n",
    "                    ham_email_count+=1\n",
    "                emails[email] = {'spam':spam,'word_count':0,'words':[]}\n",
    "            emails[email]['words'].append(word)\n",
    "            emails[email]['word_count']+=1\n",
    "\n",
    "#Calculate priors\n",
    "prior_spam=spam_email_count/len(emails)\n",
    "prior_ham=ham_email_count / len(emails)\n",
    "for k,word in words.iteritems():\n",
    "    word['p_spam']=(word['spam_count'])/spam_word_count\n",
    "    word['p_ham']=(word['ham_count'])/ham_word_count\n",
    "\n",
    "#At this point the model is now trained, and we can use it to make our predictions\n",
    "accuracy =0\n",
    "for j,email in emails.iteritems():\n",
    "    p_spam=prior_spam\n",
    "    p_ham=prior_ham\n",
    "    for word in email['words']:\n",
    "        if word in words:\n",
    "#Since there are so many words, the posteriors are going to be really low.  \n",
    "# So we need to use log to compute the posteriors.  Otherwise, we'll get underflow errors\n",
    "            try:\n",
    "                p_spam+=math.log((words[word]['p_spam']))\n",
    "                p_ham+=math.log((words[word]['p_ham']))\n",
    "            except ValueError:\n",
    "                raise #theoretically, this shouldn't happen since we have smoothing\n",
    "    if p_spam>p_ham:\n",
    "        spam_pred=1\n",
    "    else:\n",
    "        spam_pred=0\n",
    "    if (spam_pred == email['spam']): accuracy += 1\n",
    "    print j,'\\t',email['spam'],'\\t',spam_pred\n",
    "\n",
    "print \"Accuracy: \", accuracy / len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The call for HW1.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW 1.5 - Results\n",
      "0010.2003-12-18.GP \t1 \t1\n",
      "0010.2001-06-28.SA_and_HP \t1 \t1\n",
      "0001.2000-01-17.beck \t0 \t0\n",
      "0018.1999-12-14.kaminski \t0 \t0\n",
      "0005.1999-12-12.kaminski \t0 \t0\n",
      "0011.2001-06-29.SA_and_HP \t1 \t1\n",
      "0008.2004-08-01.BG \t1 \t1\n",
      "0009.1999-12-14.farmer \t0 \t0\n",
      "0017.2003-12-18.GP \t1 \t1\n",
      "0011.2001-06-28.SA_and_HP \t1 \t1\n",
      "0015.2001-07-05.SA_and_HP \t1 \t1\n",
      "0015.2001-02-12.kitchen \t0 \t0\n",
      "0009.2001-06-26.SA_and_HP \t1 \t1\n",
      "0017.1999-12-14.kaminski \t0 \t0\n",
      "0012.2000-01-17.beck \t0 \t0\n",
      "0003.2000-01-17.beck \t0 \t0\n",
      "0004.2001-06-12.SA_and_HP \t1 \t1\n",
      "0008.2001-06-12.SA_and_HP \t1 \t1\n",
      "0007.2001-02-09.kitchen \t0 \t0\n",
      "0016.2004-08-01.BG \t1 \t1\n",
      "0015.2000-06-09.lokay \t0 \t0\n",
      "0005.1999-12-14.farmer \t0 \t0\n",
      "0016.1999-12-15.farmer \t0 \t0\n",
      "0013.2004-08-01.BG \t1 \t1\n",
      "0005.2003-12-18.GP \t1 \t1\n",
      "0012.2001-02-09.kitchen \t0 \t0\n",
      "0003.2001-02-08.kitchen \t0 \t0\n",
      "0009.2001-02-09.kitchen \t0 \t0\n",
      "0006.2001-02-08.kitchen \t0 \t0\n",
      "0014.2003-12-19.GP \t1 \t1\n",
      "0010.1999-12-14.farmer \t0 \t0\n",
      "0010.2004-08-01.BG \t1 \t1\n",
      "0014.1999-12-14.kaminski \t0 \t0\n",
      "0006.1999-12-13.kaminski \t0 \t0\n",
      "0011.1999-12-14.farmer \t0 \t0\n",
      "0013.1999-12-14.kaminski \t0 \t0\n",
      "0001.2001-02-07.kitchen \t0 \t0\n",
      "0008.2001-02-09.kitchen \t0 \t0\n",
      "0007.2003-12-18.GP \t1 \t1\n",
      "0017.2004-08-02.BG \t1 \t1\n",
      "0014.2004-08-01.BG \t1 \t1\n",
      "0006.2003-12-18.GP \t1 \t1\n",
      "0016.2001-07-05.SA_and_HP \t1 \t1\n",
      "0008.2003-12-18.GP \t1 \t1\n",
      "0014.2001-07-04.SA_and_HP \t1 \t1\n",
      "0001.2001-04-02.williams \t0 \t0\n",
      "0012.2000-06-08.lokay \t0 \t0\n",
      "0014.1999-12-15.farmer \t0 \t0\n",
      "0009.2000-06-07.lokay \t0 \t0\n",
      "0001.1999-12-10.farmer \t0 \t0\n",
      "0008.2001-06-25.SA_and_HP \t1 \t1\n",
      "0017.2001-04-03.williams \t0 \t0\n",
      "0014.2001-02-12.kitchen \t0 \t0\n",
      "0016.2001-07-06.SA_and_HP \t1 \t1\n",
      "0015.1999-12-15.farmer \t0 \t0\n",
      "0009.1999-12-13.kaminski \t0 \t0\n",
      "0001.2000-06-06.lokay \t0 \t0\n",
      "0011.2004-08-01.BG \t1 \t1\n",
      "0004.2004-08-01.BG \t1 \t1\n",
      "0018.2003-12-18.GP \t1 \t1\n",
      "0002.1999-12-13.farmer \t0 \t0\n",
      "0016.2003-12-19.GP \t1 \t1\n",
      "0004.1999-12-14.farmer \t0 \t0\n",
      "0015.2003-12-19.GP \t1 \t1\n",
      "0006.2004-08-01.BG \t1 \t1\n",
      "0009.2003-12-18.GP \t1 \t1\n",
      "0007.1999-12-14.farmer \t0 \t0\n",
      "0005.2000-06-06.lokay \t0 \t0\n",
      "0010.1999-12-14.kaminski \t0 \t0\n",
      "0007.2000-01-17.beck \t0 \t0\n",
      "0003.1999-12-14.farmer \t0 \t0\n",
      "0003.2004-08-01.BG \t1 \t1\n",
      "0017.2004-08-01.BG \t1 \t1\n",
      "0013.2001-06-30.SA_and_HP \t1 \t1\n",
      "0003.1999-12-10.kaminski \t0 \t0\n",
      "0012.1999-12-14.farmer \t0 \t0\n",
      "0004.1999-12-10.kaminski \t0 \t0\n",
      "0018.2001-07-13.SA_and_HP \t1 \t1\n",
      "0002.2001-02-07.kitchen \t0 \t0\n",
      "0007.2004-08-01.BG \t1 \t1\n",
      "0012.1999-12-14.kaminski \t0 \t0\n",
      "0005.2001-06-23.SA_and_HP \t1 \t1\n",
      "0007.1999-12-13.kaminski \t0 \t0\n",
      "0017.2000-01-17.beck \t0 \t0\n",
      "0006.2001-06-25.SA_and_HP \t1 \t1\n",
      "0006.2001-04-03.williams \t0 \t0\n",
      "0005.2001-02-08.kitchen \t0 \t0\n",
      "0002.2003-12-18.GP \t1 \t1\n",
      "0003.2003-12-18.GP \t1 \t1\n",
      "0013.2001-04-03.williams \t0 \t0\n",
      "0004.2001-04-02.williams \t0 \t0\n",
      "0010.2001-02-09.kitchen \t0 \t0\n",
      "0001.1999-12-10.kaminski \t0 \t0\n",
      "0013.1999-12-14.farmer \t0 \t0\n",
      "0015.1999-12-14.kaminski \t0 \t0\n",
      "0012.2003-12-19.GP \t1 \t1\n",
      "0016.2001-02-12.kitchen \t0 \t0\n",
      "0002.2004-08-01.BG \t1 \t1\n",
      "0002.2001-05-25.SA_and_HP \t1 \t1\n",
      "0011.2003-12-18.GP \t1 \t1\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Uses same mapper and reducer as previous problem.\n",
    "#Run our HW 1.4 code and check the results in the output file\n",
    "!chmod a+x mapper.py reducer.py\n",
    "!./pNaiveBayes.sh 5 \n",
    "!echo \"HW 1.5 - Results\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.6 Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  Training Error\n",
      "1    Bernoulli NB            0.16\n",
      "2  Multinomial NB            0.00\n",
      "3     HW1.5 Model            0.00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "data = np.array([])\n",
    "labels = np.array([])\n",
    "opened = open (\"enronemail_1h.txt\", \"r\") \n",
    "for line in opened.readlines():\n",
    "        [email,spam,subject,content] = re.split(\"\\t\",line)            \n",
    "        text = content.lower() + subject.lower()\n",
    "        data=np.append(data,[text])\n",
    "        labels=np.append(labels,[spam])\n",
    "        \n",
    "# Create features for train and dev data \n",
    "vectorizer = CountVectorizer()\n",
    "trainingData = vectorizer.fit_transform(data)\n",
    "#Data frame that I will use to show training errors\n",
    "d = pd.DataFrame({\"Model\":[],\"Training Error\":[]})\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(trainingData, labels) \n",
    "#Training error is 1- accuracy score\n",
    "d.loc[1]=[\"Bernoulli NB\", 1-classifier.score(trainingData,labels)]\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(trainingData, labels) \n",
    "d.loc[2]=[\"Multinomial NB\", 1-classifier.score(trainingData,labels)]\n",
    "#I'm hardcoding the error rate I got from HW1.5\n",
    "d.loc[3]=[\"HW1.5 Model\", 0]\n",
    "\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn Multinomial NB mode and the model developed in HW 1.5 model both perform much better than the sklearn Bernoulli NB classifier.  This is likely because they are both Multinomial models while the Bernoulli model is a different algorithm.  I think that Multinomial models take into account the frequency of each class in the training data, meaning that it will generally bias more towards ham than spam. \n",
    "\n",
    "The training error between the sklearn Multinomial and the HW 1.5 model is the same.  This makes sense because the two models and feature sets should theoretically be the same.  Both parse words based on spaces and then use log addition to calculate posterior probabilities. Both also use +1 smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
