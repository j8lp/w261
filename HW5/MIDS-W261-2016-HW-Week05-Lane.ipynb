{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 5\n",
    "Jackson Lane (jelane@berkeley.edu) <br>\n",
    "W261-3 <br>\n",
    "\n",
    "\n",
    "# === Week 5 ASSIGNMENTS ===\n",
    "\n",
    "## HW 5.0\n",
    "#### What is a data warehouse? \n",
    "A data warehouse is repository storing structured data used for reporting and analytics.  It's kind of like a database that is used for analytics instead of transactions (OLAP vs OLTP).  Often a data warehouse pulls data or is built on top of one or more other databases or data warehouses.  However, data warehouses are structured, so they have to extract and transform data prior to storing it.  The benefit of this is that users do not need to deal with the different naming conventions and data models of the backend systems and instead can write queries against one universal semantic view.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "#### In the database world What is 3NF? \n",
    "\n",
    "Does machine learning use data in 3NF? If so why? \n",
    "\n",
    "#### In what form does ML consume data?\n",
    "Why would one use log files that are denormalized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "#### Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "I chose the URLs table as the left table, as it's smaller than the Visits table.  Although, not byy much, as wh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right\n",
    "\n",
    "(2) Right joining Table Left with Table Right\n",
    "\n",
    "(3) Inner joining Table Left with Table Right\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_2.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import re\n",
    "\n",
    "class MRJob5_2(MRJob):\n",
    "    \n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper = self.mapper, \n",
    "                mapper_final = self.mapper_final, jobconf={\n",
    "                 'mapreduce.job.maps': '1'\n",
    "                          })]\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        self.urls = {}\n",
    "        self.pages = []\n",
    "        self.all = 0\n",
    "        self.rightonly = 0\n",
    "        self.leftonly = 0\n",
    "        with open(\"urls.data\") as urls:\n",
    "            for line in urls.readlines():\n",
    "                fields = line.strip().split(\",\")\n",
    "                self.urls[fields[1]] = \"www.microsoft.com\" + fields[4].strip().strip('\"') \n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        line = line.split(\",\")\n",
    "        webpageID = line[1]\n",
    "        visitorID = line[4]\n",
    "        self.pages.append(webpageID)\n",
    "        if webpageID in self.urls.keys():\n",
    "            self.all += 1\n",
    "            yield \"inner\",self.urls[webpageID]+\",\"+webpageID+\",\"+visitorID\n",
    "            yield \"left\",self.urls[webpageID]+\",\"+webpageID+\",\"+visitorID\n",
    "            yield \"right\",self.urls[webpageID]+\",\"+webpageID+\",\"+visitorID\n",
    "        else:\n",
    "            self.rightonly += 1\n",
    "            yield \"right\",\"None,\"+webpageID+\",\"+visitorID\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        #Let's now check if there are any pages that never got any visitors, meaning that only a left join is possible\n",
    "        for webpageID in self.urls.keys():\n",
    "            if webpageID not in self.pages:\n",
    "                self.leftonly += 1\n",
    "                yield \"left\", self.urls[webpageID]+\",\"+webpageID+\",None\"\n",
    "        yield \"Number of Possible Left Joins\",str(self.all + self.leftonly)\n",
    "        yield \"Number of Possible Inner Joins\", str(self.all)\n",
    "        yield \"Number of Possible Right Joins\",str(self.all + self.rightonly)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJob5_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Driver Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"left\"\t\"www.microsoft.com/devmovies,1290,None\"\r\n",
      "\"left\"\t\"www.microsoft.com/news,1291,None\"\r\n",
      "\"left\"\t\"www.microsoft.com/centroam,1297,None\"\r\n",
      "\"left\"\t\"www.microsoft.com/bookshelf,1294,None\"\r\n",
      "\"left\"\t\"www.microsoft.com/autoroute,1287,None\"\r\n",
      "\"left\"\t\"www.microsoft.com/masterchef,1289,None\"\r\n",
      "\"left\"\t\"www.microsoft.com/library,1288,None\"\r\n",
      "\"Number of Left Joins\"\t\"98663\"\r\n",
      "\"Number of Inner Joins\"\t\"98654\"\r\n",
      "\"Number of Right Joins\"\t\"98654\"\r\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from MRJob5_2 import MRJob5_2\n",
    "mr_job = MRJob5_2(args=['Processed-anonymous-msweb.data','-r','hadoop','--file',\"urls.data\",'--cleanup'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    output = open(\"output.txt\",\"w\")\n",
    "    for line in runner.stream_output():\n",
    "        output.write(line)\n",
    "    output.close()\n",
    "!tail -10 output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_3.py\n",
    "#!/usr/bin/python\n",
    "#\n",
    "# This job will read a a series of NGrams and produce the following summmary statistics\n",
    "# Longest NGram\n",
    "# Top 10 most frequent words\n",
    "# 20 most densley appearing words\n",
    "# 20 least densley appearing words\n",
    "# Distribution of NGram sizes (character count)\n",
    "# This job computes these summary statistics across 5 different partitions one for each summary statistic\n",
    "# The reducer code is set to behave differently depending on which partition it is in\n",
    "# Overall running time for me was several hours ,\n",
    "# most likely due to the fact that the first step in this job emits each line of text \n",
    "# 5 times (once for each word in the 5 gram)/\n",
    "\n",
    "import sys\n",
    "from mrjob.job import MRJob, MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRJob5_3(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.mapper_wordcount, combiner = self.combiner_wordcount, reducer = self.reducer_wordcount),\n",
    "               MRStep(mapper = self.mapper_process, reducer = self.reducer_process, jobconf={\n",
    "                                        'mapreduce.job.maps': '8',\n",
    "                    'mapreduce.job.reduces': '4',\n",
    "                            }),\n",
    "                MRStep(reducer_init = self.reducer_sortreport_init, reducer = self.reducer_sortreport, \n",
    "                jobconf={\n",
    "                    'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'stream.num.map.output.key.field': 3,\n",
    "                    'stream.map.output.field.separator':'\\t',\n",
    "                    'mapreduce.partition.keycomparator.options': '-k1,1 -k2,2n',\n",
    "                          })]         \n",
    "                      \n",
    "    def mapper_wordcount(self, _, line):\n",
    "            line = line.strip()\n",
    "            [ngram,count,pages_count,books_count] = line.split(\"\\t\")\n",
    "            count = int(count)\n",
    "            pages_count = int(pages_count)\n",
    "            books_count = int(books_count)\n",
    "            words = ngram.split()\n",
    "            for word in words:\n",
    "                yield ngram+\",\"+word, str([count,pages_count,books_count])\n",
    "\n",
    "    def combiner_wordcount(self, key, counts):\n",
    "        newcount =0\n",
    "        for line in counts: \n",
    "            [count,pages_count,books_count] =eval(line)\n",
    "            newcount += count\n",
    "        yield key,str([newcount,pages_count,books_count])\n",
    "\n",
    "    def reducer_wordcount(self, key, counts):\n",
    "        newcount =0\n",
    "        for line in counts:\n",
    "            [count,pages_count,books_count] =eval(line)\n",
    "            newcount += count\n",
    "        yield key,str([newcount,pages_count,books_count] )\n",
    "            \n",
    "    def mapper_process  (self,key,counts):\n",
    "        ngram, word = key.split(\",\")\n",
    "        words_in_ngram = len(ngram.split())\n",
    "        [count,pages_count,books_count] = eval(counts)\n",
    "        yield \"A: Longest 5-gram\\t1\", ngram\n",
    "        yield \"B: Top 10 most frequent words,\" + word+\"\\t1\",str(count)\n",
    "        yield \"C: 20 Most/Least densely appearing words,\" + word+\"\\t1\",str([count,pages_count])\n",
    "        yield \"D: Distribution of 5-gram sizes,\" + str( len(ngram))+\"\\t1\",str(float(count) / words_in_ngram)\n",
    "    \n",
    "    def reducer_process(self,key,values):\n",
    "        keyfields = key.split(\",\")\n",
    "        goal = keyfields[0]\n",
    "        values = map(lambda v: v.split(\"\\t\")[1], values)\n",
    "\n",
    "        if goal == \"A: Longest 5-gram\":\n",
    "            ngrams = values\n",
    "            for ngram in ngrams:\n",
    "                yield goal, str(-1*len(ngram))+\"\\t\"+ngram\n",
    "        elif goal == \"B: Top 10 most frequent words\":\n",
    "            word = keyfields[1]\n",
    "            yield goal,str(-1*sum(map(int,values)))+\"\\t\"+word\n",
    "        elif goal == \"C: 20 Most/Least densely appearing words\" :\n",
    "            word = keyfields[1]\n",
    "            count =0\n",
    "            pages_count = 0\n",
    "            for line in values:\n",
    "                [c,p] = eval(line)\n",
    "                count+= c\n",
    "                pages_count+=p\n",
    "            yield \"C: 20 least densely appearing words\",str(sum([(float(1)*count)/ pages_count]))+\"\\t\"+word\n",
    "            yield \"C: 20 most densely appearing words\",str(sum([(float(-1)*count)/ pages_count]))+\"\\t\"+word\n",
    "        else:\n",
    "            gram_length = keyfields[1]\n",
    "            yield \"D: Distribution of 5-gram sizes\",str(sum(map(float,values)))+\"\\t\"+gram_length\n",
    "            \n",
    "    def reducer_sortreport_init(self):\n",
    "        self.goal_report_limits = {\n",
    "            \"A: Longest 5-gram\": 1,\n",
    "            \"B: Top 10 most frequent words\": 10,\n",
    "            \"C: 20 least densely appearing words\": 20,\n",
    "            \"C: 20 most densely appearing words\": 20,\n",
    "            \"D: Distribution of 5-gram sizes\": -1\n",
    "        }\n",
    "    \n",
    "    def reducer_sortreport(self,goal,lines):\n",
    "        yield \"--------\",\"--------\"\n",
    "        yield goal, \":\"\n",
    "        for line in lines:\n",
    "            if self.goal_report_limits[goal] == 0: break\n",
    "            self.goal_report_limits[goal] = self.goal_report_limits[goal] -1\n",
    "            line = line.strip()\n",
    "            number_field,category_field = line.split(\"\\t\")\n",
    "            number_field = float(number_field)\n",
    "            if number_field < 0: number_field *= -1\n",
    "            yield category_field, str(number_field)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJob5_3.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 10 filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt > unitTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/19 15:11:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/19 15:12:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/5.3/UnitTest\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.3/UnitTest\n",
    "!python MRJob5_3.py unitTestData -r hadoop --output-dir ./results/5.3/UnitTest \\\n",
    "--cleanup=ALL 2>/dev/null >NGramsEDAUnitTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\t--------\n",
      "A: Longest 5-gram\t:\n",
      "A Circumstantial Narrative of the\t33.0\n",
      "--------\t--------\n",
      "B: Top 10 most frequent words\t:\n",
      "A\t2217.0\n",
      "in\t1201.0\n",
      "Christmas\t1099.0\n",
      "Wales\t1099.0\n",
      "Child's\t1099.0\n",
      "of\t1011.0\n",
      "Study\t604.0\n",
      "Case\t604.0\n",
      "Female\t447.0\n",
      "Collection\t239.0\n",
      "--------\t--------\n",
      "C: 20 least densely appearing words\t:\n",
      "Female\t1.0\n",
      "Case\t1.0\n",
      "Limited\t1.0\n",
      "Narrative\t1.0\n",
      "ESTABLISHING\t1.0\n",
      "RELIGIOUS\t1.0\n",
      "Government\t1.0\n",
      "Circumstantial\t1.0\n",
      "FOR\t1.0\n",
      "Study\t1.0\n",
      "BILL\t1.0\n",
      "the\t1.01639344262\n",
      "George\t1.02222222222\n",
      "General\t1.02222222222\n",
      "Biography\t1.02222222222\n",
      "A\t1.02829313544\n",
      "in\t1.03267411866\n",
      "Sea\t1.03333333333\n",
      "City\t1.03333333333\n",
      "by\t1.03333333333\n",
      "--------\t--------\n",
      "C: 20 most densely appearing words\t:\n",
      "of\t1.15675057208\n",
      "Forms\t1.12621359223\n",
      "Collection\t1.08636363636\n",
      "Fairy\t1.05128205128\n",
      "Tales\t1.05128205128\n",
      "Child's\t1.03581526861\n",
      "Wales\t1.03581526861\n",
      "Christmas\t1.03581526861\n",
      "by\t1.03333333333\n",
      "City\t1.03333333333\n",
      "Sea\t1.03333333333\n",
      "in\t1.03267411866\n",
      "A\t1.02829313544\n",
      "George\t1.02222222222\n",
      "Biography\t1.02222222222\n",
      "General\t1.02222222222\n",
      "the\t1.01639344262\n",
      "Study\t1.0\n",
      "BILL\t1.0\n",
      "FOR\t1.0\n",
      "--------\t--------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff66f0c8450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "distribution = {}\n",
    "with open(\"NGramsEDAUnitTest\") as myfile:\n",
    "    partD = False\n",
    "    for line in myfile.readlines():\n",
    "        line = line.strip().replace('\"','')\n",
    "        line =  line.split(\"\\t\")\n",
    "        if (line[0] == \"D: Distribution of 5-gram sizes\"):\n",
    "            partD = True\n",
    "            continue\n",
    "        if partD:\n",
    "            distribution[int(line[0])] = float(line[1])\n",
    "        else: print '\\t'.join(map(str,line))\n",
    "\n",
    "plt.hist(distribution.keys(), weights=distribution.values(), bins=50)\n",
    "plt.title(\"Histogram of 5-gram sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the job on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/19 15:29:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/19 15:29:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/5.3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.3\n",
    "!python MRJob5_3.py filtered-5Grams -r hadoop --output-dir ./results/5.3 \\\n",
    "--cleanup=ALL 2>/dev/null >NGramsEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\t--------\n",
      "A: Longest 5-gram\t:\n",
      "AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\t160.0\n",
      "--------\t--------\n",
      "B: Top 10 most frequent words\t:\n",
      "the\t5375699242.0\n",
      "of\t3691308874.0\n",
      "to\t2221164346.0\n",
      "in\t1387638591.0\n",
      "a\t1342195425.0\n",
      "and\t1135779433.0\n",
      "that\t798553959.0\n",
      "is\t756296656.0\n",
      "be\t688053106.0\n",
      "as\t481373389.0\n",
      "--------\t--------\n",
      "C: 20 least densely appearing words\t:\n",
      "Leshchetitsky\t1.0\n",
      "Leshkowich\t1.0\n",
      "Nescit\t1.0\n",
      "Leskov\t1.0\n",
      "Nescio\t1.0\n",
      "Leslea\t1.0\n",
      "Nescience\t1.0\n",
      "Nescia\t1.0\n",
      "Lesley's\t1.0\n",
      "Nescafe\t1.0\n",
      "Leslies\t1.0\n",
      "Nesbitt\t1.0\n",
      "Lesly\t1.0\n",
      "Nesbit's\t1.0\n",
      "Lesly's\t1.0\n",
      "Nesace\t1.0\n",
      "Lesmahago\t1.0\n",
      "Lespinasse\t1.0\n",
      "Lespenard\t1.0\n",
      "AAAA\t1.0\n",
      "--------\t--------\n",
      "C: 20 most densely appearing words\t:\n",
      "xxxx\t11.557291666666666\n",
      "NA\t10.161726044782885\n",
      "blah\t8.0741599073001158\n",
      "nnn\t7.5333333333333332\n",
      "nd\t6.5611436445056839\n",
      "ND\t5.4073642846747196\n",
      "oooooooooooooooo\t4.921875\n",
      "PIC\t4.7272727272727275\n",
      "llll\t4.5116279069767442\n",
      "LUTHER\t4.3494983277591972\n",
      "oooooo\t4.2072378595731514\n",
      "NN\t4.0908402725208175\n",
      "ooooo\t3.9492846924177396\n",
      "OOOOOO\t3.9313725490196076\n",
      "IIII\t3.7877030162412995\n",
      "lillelu\t3.7624521072796937\n",
      "OOOOO\t3.6570701447431206\n",
      "Sc\t3.6065624999999999\n",
      "Pfeffermann\t3.5769230769230771\n",
      "Madarassy\t3.5769230769230771\n",
      "--------\t--------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff66e37c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "distribution = {}\n",
    "with open(\"NGramsEDA\") as myfile:\n",
    "    partD = False\n",
    "    for line in myfile.readlines():\n",
    "        line = line.strip().replace('\"','')\n",
    "        line =  line.split(\"\\t\")\n",
    "        if (line[0] == \"D: Distribution of 5-gram sizes\"):\n",
    "            partD = True\n",
    "            continue\n",
    "        if partD:\n",
    "            distribution[int(line[0])] = float(line[1])\n",
    "        else: print '\\t'.join(map(str,line))\n",
    "\n",
    "plt.hist(distribution.keys(), weights=distribution.values(), bins=50)\n",
    "plt.title(\"Histogram of 5-gram sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW 5.3.1 OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "https://en.wikipedia.org/wiki/Power_law\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "with open(\"stopwords\",\"w\") as myfile:\n",
    "    myfile.write(str(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the stopwords from seem to all be in lowercase, I'll the ngram words to lowercase as well before checking whether they exist in the stopwords list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_wordcount.py\n",
    "from mrjob.job import MRJob, MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRJob5_4_wordcount(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_wordcount_init,mapper = self.mapper_wordcount, \n",
    "                       combiner = self.reducer_wordcount, reducer = self.reducer_wordcount),\n",
    "                MRStep(reducer = self.reducer_sortreport,\n",
    "                jobconf={\n",
    "               'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':'\\t',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "\n",
    "                          })]         \n",
    "                      \n",
    "    def mapper_wordcount_init(self):\n",
    "            with open('stopwords') as myfile:\n",
    "                self.stopwords = eval(myfile.readline())\n",
    "                \n",
    "    def mapper_wordcount(self, _, line):\n",
    "            line = line.strip()\n",
    "            [ngram,count,pages_count,books_count] = line.split(\"\\t\")\n",
    "            words = ngram.split()\n",
    "            for word in words:\n",
    "                #Filter out the stop words\n",
    "                if word.lower() not in self.stopwords:\n",
    "                    yield word, count\n",
    "\n",
    "    def reducer_wordcount(self, key, counts):\n",
    "        yield key,str(sum(map(int,counts)))\n",
    "        \n",
    "    def reducer_sortreport(self,key,counts):\n",
    "              yield key, counts.next()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJob5_4_wordcount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/19 18:05:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `results/5.4/wordcount': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!hdfs dfs -rm -r results/5.4/wordcount\n",
    "\n",
    "from MRJob5_4_wordcount import MRJob5_4_wordcount\n",
    "mr_job = MRJob5_4_wordcount(args=['filtered-5Grams','-r','hadoop',\n",
    "                                  '--file','stopwords','--output-dir','./results/5.4/wordcount','--cleanup','ALL'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    output = open(\"topwords.txt\",\"w\")\n",
    "    for line in runner.stream_output():\n",
    "        output.write(line)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_cooccurence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_cooccurence.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import sys\n",
    "\n",
    "class MRJob5_4_cooccurence(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    def steps(self):\n",
    "           return [  MRStep(mapper_init=self.mapper_coocurrence_init, mapper=self.mapper_coocurrence\n",
    "                        , combiner=self.combiner_coocurrence, reducer_init=self.reducer_coocurrence_init\n",
    "                        , reducer=self.reducer_coocurrence, reducer_final=self.reducer_coocurrence_final\n",
    "                        , jobconf={\n",
    "                        'mapreduce.job.reduces': '4', \n",
    "                    } \n",
    "                      ),\n",
    "                       MRStep(reducer = self.reducer_merge)\n",
    "                  ]\n",
    "    \n",
    "\n",
    "    def mapper_coocurrence_init(self):\n",
    "        with open(\"topwords.txt\") as myfile:\n",
    "            vocab = []\n",
    "            for i in range(10000):\n",
    "                line = myfile.readline().strip()\n",
    "                word, count = line.split(\"\\t\")\n",
    "                vocab.append(word.strip())\n",
    "            self.top10k = set(vocab[:10001])\n",
    "            self.basis =  set(vocab[9001:10001])\n",
    "        \n",
    "      \n",
    "    def mapper_coocurrence(self, _, line):\n",
    "        # parse line, get words and counts\n",
    "        ngram, count, _, _ = line.strip().split('\\t')\n",
    "        words = ngram.split()\n",
    "        words = sorted(list(set(filter(lambda w: w in self.top10k,words))))\n",
    "        # emit each pair of words\n",
    "        for word1 in words:\n",
    "            for word2 in words:\n",
    "                if word2 in self.basis and word2 <> word1:\n",
    "                    yield [word1,word2],int(count)\n",
    "\n",
    "    def combiner_coocurrence(self, words, counts):\n",
    "        yield words, sum(map(int,counts))\n",
    "    \n",
    "    def reducer_coocurrence_init(self):\n",
    "        self.word = '*'\n",
    "        self.stripe = {}\n",
    "                        \n",
    "    def reducer_coocurrence(self, words, counts):    \n",
    "        newword1, newword2 = words    \n",
    "        sys.stderr.write(str(words) + \"\\n\")\n",
    "        if self.word == newword1:\n",
    "            self.stripe[newword2] =  self.stripe.setdefault(newword2,0)+ sum(counts)\n",
    "        else:\n",
    "            if len(self.stripe) > 0:\n",
    "                yield self.word, self.stripe\n",
    "            self.word = newword1\n",
    "            self.stripe = {newword2:sum(counts)}\n",
    "            \n",
    "\n",
    "    def reducer_coocurrence_final(self):\n",
    "        if len(self.stripe) > 0:\n",
    "            yield self.word, self.stripe\n",
    "    \n",
    "        \n",
    "    def reducer_merge(self,word,dicts):\n",
    "        result = {}\n",
    "        for d in dicts:\n",
    "                for key,value in d.items():\n",
    "                    result[key] = result.setdefault(key,0) + int(value)\n",
    "\n",
    "        yield word, result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJob5_4_cooccurence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_invert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_invert.py\n",
    "from DebuggerProtocol import DebuggerProtocol\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class MRJob5_4_invert(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "    INTERNAL_PROTOCOL = DebuggerProtocol(JSONProtocol)\n",
    "    def steps(self):\n",
    "           return [  \n",
    "                   MRStep( mapper=self.mapper_invert,reducer_init=self.reducer_invert_init\n",
    "                    , reducer=self.reducer_invert\n",
    "                    , jobconf={\n",
    "                    'mapreduce.job.reduces': 4,\n",
    "                    } \n",
    "                      )\n",
    "                  ]\n",
    "    \n",
    "\n",
    "    def mapper_invert(self, _, line):\n",
    "        word1, stripe = line.split(\"\\t\")\n",
    "        stripe = eval(stripe)\n",
    "        yield word1,('*',len(stripe))\n",
    "        for word2,count in stripe.items():\n",
    "                    yield word2, (word1+'.'+str(len(stripe)),count)\n",
    "                        \n",
    "    def reducer_invert_init(self):\n",
    "        self.length = 0\n",
    "        self.stripe={}\n",
    "        \n",
    "    def reducer_invert(self,word1, word_counts):\n",
    "        for (word2,count) in word_counts:\n",
    "            word2 = str(word2)\n",
    "            count = int(count)\n",
    "            if(word2 == '*'):\n",
    "                self.length = count\n",
    "            else:\n",
    "                self.stripe[word2] =  self.stripe.setdefault(word2,0)+ count\n",
    "        if len(self.stripe) > 0:\n",
    "            yield word1 +'.'+str(self.length), self.stripe\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRJob5_4_invert.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similiarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_jaccard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_jaccard.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import sys\n",
    "class MRJob5_4_jaccard(MRJob):\n",
    "    MRJob.SORT_VALUES = True \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_jaccard , reducer=self.reducer_jaccard, jobconf= \n",
    "                        {           \n",
    "            'mapreduce.job.reduces': '4',\n",
    "                })\n",
    "   \n",
    "    def mapper_jaccard(self, _, line):\n",
    "        word0, stripe = line.split(\"\\t\")\n",
    "        stripe = eval(stripe).keys()\n",
    "        for i,word1 in enumerate(stripe):\n",
    "            for word2 in stripe[i+1:]:\n",
    "                yield (word1,word2), 1\n",
    "        \n",
    "    def reducer_jaccard(self,words,counts):\n",
    "        word1,word2 = map(str,words)\n",
    "        word1, length1 = word1.split(\".\")\n",
    "        word2,length2 = word2.split(\".\")\n",
    "        intersection = sum(map(float,counts))\n",
    "        jaccard = intersection / (int(length1) + int(length2) - intersection)\n",
    "        yield (word1,word2), jaccard\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRJob5_4_jaccard.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_cosine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_cosine.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import sys\n",
    "\n",
    "class MRJob5_4_cosine(MRJob):\n",
    "    MRJob.SORT_VALUES = True \n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            #The first step calculates the inverted index and normalizes\n",
    "            MRStep(\n",
    "                mapper=self.mapper_inverted\n",
    "                ,combiner=self.combiner_inverted\n",
    "                ,reducer=self.reducer_inverted,\n",
    "                jobconf= \n",
    "                        {           \n",
    "            'mapreduce.job.reduces': '4',\n",
    "                }) ,\n",
    "            MRStep(\n",
    "                mapper=self.mapper_cosine\n",
    "                ,reducer=self.reducer_cosine,\n",
    "                jobconf= \n",
    "                        {           \n",
    "            'mapreduce.partition.keycomparator.options': '-k2',            \n",
    "            'mapreduce.job.reduces': '4',\n",
    "                })\n",
    "                  \n",
    "        ]\n",
    "    \n",
    "    def mapper_inverted(self, _, line):\n",
    "        line = line.strip()\n",
    "        word1, stripe=line.split('\\t')\n",
    "        stripe=eval(stripe)\n",
    "        length = len(stripe)\n",
    "        for (word2, count) in stripe.items():\n",
    "            #Normalize length so that sum of squares is 1\n",
    "            yield word2, {word1: 1/(float(length) ** .5)}\n",
    "\n",
    "    def combiner_inverted(self, word, dicts):\n",
    "        result = {}\n",
    "        for d in dicts:\n",
    "            result.update(d)\n",
    "        yield word, result\n",
    "        \n",
    "\n",
    "    def reducer_inverted(self,word2,dicts):\n",
    "        result={}\n",
    "        for d in dicts:\n",
    "            for (word1,count) in d.items():\n",
    "                result[word1] = result.setdefault(word1,0)+int(count)\n",
    "        yield word2, result\n",
    "\n",
    "    def mapper_cosine(self,word,stripe):\n",
    "        words=stripe.keys()     \n",
    "        sys.stderr.write(str(words)+\"\\n\")\n",
    "        for i,word1 in enumerate(words[:-1]):\n",
    "            for word2 in words[i+1:]:\n",
    "                #Compute dot product\n",
    "                yield (word1,word2),stripe.setdefault(word1,0)*stripe.setdefault(word2,0)\n",
    "        \n",
    "    def reducer_cosine(self,words,counts):\n",
    "        yield words,sum(counts)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRJob5_4_cosine.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's run the System Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SystemTestData\n"
     ]
    }
   ],
   "source": [
    "%%writefile SystemTestData\n",
    "DocA\t{'X':20, 'Y':30, 'Z':5}\n",
    "DocB\t{'X':100, 'Y':20}\n",
    "DocC\t{'M':5, 'N':20, 'Z':5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data already appears to be indexed, we don't need to run the cooccurenceUnitTest job.  Instead, let's just get straight to the invertedindex job:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/22 01:44:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/22 01:44:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/5.4/UnitTest/invertedIndex\n",
      "\"M.3\"\t{\"DocC.3\": 5}\n",
      "\"N.3\"\t{\"DocC.3\": 25}\n",
      "\"X.3\"\t{\"DocA.3\": 20, \"DocC.3\": 25}\n",
      "\"Y.0\"\t{\"DocB.2\": 20}\n",
      "\"Z.0\"\t{\"DocB.2\": 20, \"DocA.3\": 5}\n",
      "\"Y.3\"\t{\"DocA.3\": 30}\n",
      "\"X.0\"\t{\"DocB.2\": 100}\n",
      "\"Z.0\"\t{\"DocB.2\": 100, \"DocC.3\": 5}\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/UnitTest/invertedIndex\n",
    "!python MRJob5_4_invert.py SystemTestData -r hadoop --file DebuggerProtocol.py --output-dir ./results/5.4/UnitTest/invertedIndex \\\n",
    "--cleanup=ALL  2>/dev/null >invertedIndex\n",
    "!head -25 invertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next run a unit test on the data from the Google Group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting UnitTestData\n"
     ]
    }
   ],
   "source": [
    "%%writefile UnitTestData\n",
    "advisory bedside\t50\t50\t50\n",
    "bedside Cathedral disciplines\t10\t10\t10\n",
    "advisory disciplines America\t15\t15\t15\n",
    "irrespective disciplines America\t15\t15\t15\n",
    "America AB\t15\t15\t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/22 01:45:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `results/5.4/UnitTest/co-occurence': No such file or directory\n",
      "\"America\"\t{\"advisory\": 15, \"AB\": 15, \"disciplines\": 30, \"irrespective\": 15}\n",
      "\"Cathedral\"\t{\"bedside\": 10, \"disciplines\": 10}\n",
      "\"advisory\"\t{\"bedside\": 50, \"disciplines\": 15}\n",
      "\"bedside\"\t{\"Cathedral\": 10, \"disciplines\": 10, \"advisory\": 50}\n",
      "\"disciplines\"\t{\"Cathedral\": 10, \"bedside\": 10, \"advisory\": 15, \"irrespective\": 15}\n",
      "\"irrespective\"\t{\"disciplines\": 15}\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/UnitTest/co-occurence\n",
    "!python MRJob5_4_cooccurence.py UnitTestData -r hadoop --file topwords.txt --output-dir ./results/5.4/UnitTest/co-occurence \\\n",
    "--cleanup=ALL  2>/dev/null >cooccurenceUnitTest\n",
    "!head -25 cooccurenceUnitTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/22 01:56:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `results/5.4/UnitTest/invertedIndex': No such file or directory\n",
      "\"Cathedral.0\"\t{\"\\\"disciplines\\\".4\": 10, \"\\\"bedside\\\".3\": 10}\n",
      "\"\\\"America\\\".4\"\t{\"\\\"disciplines\\\".4\": 10, \"\\\"bedside\\\".3\": 10}\n",
      "\"\\\"Cathedral\\\".2\"\t{\"\\\"disciplines\\\".4\": 10, \"\\\"bedside\\\".3\": 10}\n",
      "\"advisory.2\"\t{\"\\\"disciplines\\\".4\": 25, \"\\\"bedside\\\".3\": 10}\n",
      "\"bedside.2\"\t{\"\\\"disciplines\\\".4\": 35, \"\\\"bedside\\\".3\": 10}\n",
      "\"disciplines.2\"\t{\"\\\"advisory\\\".2\": 15, \"\\\"disciplines\\\".4\": 35, \"\\\"bedside\\\".3\": 10, \"\\\"Cathedral\\\".2\": 10}\n",
      "\"advisory.4\"\t{\"\\\"bedside\\\".3\": 50}\n",
      "\"disciplines.4\"\t{\"\\\"irrespective\\\".1\": 15, \"\\\"bedside\\\".3\": 60}\n",
      "\"irrespective.4\"\t{\"\\\"America\\\".4\": 15, \"\\\"irrespective\\\".1\": 15, \"\\\"bedside\\\".3\": 60}\n",
      "\"bedside.1\"\t{\"\\\"advisory\\\".2\": 50}\n",
      "\"disciplines.1\"\t{\"\\\"America\\\".4\": 30, \"\\\"advisory\\\".2\": 50}\n",
      "\"irrespective.1\"\t{\"\\\"America\\\".4\": 30, \"\\\"advisory\\\".2\": 50, \"\\\"disciplines\\\".4\": 15}\n",
      "\"AB.0\"\t{\"\\\"America\\\".4\": 15}\n",
      "\"\\\"advisory\\\".2\"\t{\"\\\"America\\\".4\": 15}\n",
      "\"\\\"bedside\\\".3\"\t{\"\\\"America\\\".4\": 15}\n",
      "\"advisory.3\"\t{\"\\\"America\\\".4\": 30}\n",
      "\"bedside.3\"\t{\"\\\"America\\\".4\": 30, \"\\\"Cathedral\\\".2\": 10}\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/UnitTest/invertedIndex\n",
    "!python MRJob5_4_invert.py cooccurenceUnitTest -r hadoop --file DebuggerProtocol.py --output-dir ./results/5.4/UnitTest/invertedIndex \\\n",
    "--cleanup=ALL  2>/dev/null >invertedIndex\n",
    "!head -25 invertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/22 01:41:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/22 01:41:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/5.4/UnitTest/jaccard\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/UnitTest/jaccard\n",
    "!python MRJob5_4_jaccard.py in -r hadoop  --output-dir ./results/5.4/UnitTest/jaccard \\\n",
    "--cleanup=ALL 2>/dev/null | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/22 01:51:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/22 01:51:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/5.4/UnitTest/cosine\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/UnitTest/cosine\n",
    "!python MRJob5_4_cosine.py cooccurenceUnitTest -r hadoop  --output-dir ./results/5.4/UnitTest/cosine \\\n",
    "--cleanup=ALL 2>/dev/null | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run on NGram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/21 20:29:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `results/5.4/co-occurence': No such file or directory\n",
      "\"AB\"\t{\"wires\": 51, \"carriers\": 53, \"Nuclear\": 53, \"Price\": 92, \"diagonal\": 58, \"CD\": 15062, \"Oriental\": 133, \"segregation\": 77, \"lever\": 77, \"Fleet\": 50, \"Type\": 48, \"honors\": 43}\n",
      "\"AD\"\t{\"AB\": 309, \"Geography\": 140, \"Seventh\": 166, \"diagonal\": 64, \"CD\": 53, \"Peninsula\": 343, \"Buddhism\": 70, \"Towards\": 78, \"Documents\": 95, \"embassy\": 42, \"Trent\": 52, \"Throughout\": 57, \"HISTORY\": 51}\n",
      "\"AIDS\"\t{\"alveolar\": 52, \"Memorial\": 50, \"Task\": 1152, \"carriers\": 245, \"Nervous\": 457, \"Seventh\": 620, \"epidemic\": 14055, \"Practical\": 92, \"immunodeficiency\": 257, \"Swiss\": 59, \"abnormalities\": 54, \"degeneration\": 73, \"Changes\": 55, \"Geography\": 118}\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/co-occurence\n",
    "!python MRJob5_4_cooccurence.py filtered-5Grams -r hadoop --file topwords.txt --output-dir ./results/5.4/co-occurence \\\n",
    "--cleanup=ALL 2> /dev/null > cooccurence\n",
    "!head -3 cooccurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/22 01:57:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/22 01:57:44 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted results/5.4/jaccard\n",
      "/bin/sh: /d: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r results/5.4/jaccard\n",
    "!python MRJob5_4_jaccard.py 'cooccurence'  -r hadoop --output-dir ./results/5.4/jaccard \\\n",
    "--cleanup=ALL 2>/null/dev > 'jaccard' \n",
    "!head -25 jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r results/5.4/cosine\n",
    "!python MRJob5_4_jaccard.py 'cooccurence'  -r hadoop --output-dir ./results/5.4/jaccard \\\n",
    "--cleanup=ALL 2>/dev/null > 'cosine' \n",
    "!head -25 cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.5 \n",
    "Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in HW5.4, and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python2.7\n",
    "''' pass a string to this funciton ( eg 'car') and it will give you a list of\n",
    "words which is related to cat, called lemma of CAT. '''\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import numpy as np\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Bachelor_of_Arts',\n",
       " u'abdominal',\n",
       " u'BA',\n",
       " u'type_AB',\n",
       " u'AB',\n",
       " u'Artium_Baccalaurens',\n",
       " u'group_AB',\n",
       " u'Av',\n",
       " u'abdominal_muscle',\n",
       " u'ab',\n",
       " u'Ab']"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms(\"AB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW5.6 Optional\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall\n",
    "\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "HW 5.7 (optional)\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    ">> from nltk.corpus import stopwords\n",
    ">>> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "HW 5.6 (optional)\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams.\n",
    "--\n",
    "\n",
    "Hw 5.7 (optional)\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?\n",
    "\n",
    "--\n",
    "\n",
    "=====================\n",
    "END OF ASSIGNMENT #5\n",
    "====================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
